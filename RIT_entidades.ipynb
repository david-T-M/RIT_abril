{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/rit/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils as ut # esta librería tiene funciones para poder obtener un procesamiento del <T,H>\n",
    "import spacy\n",
    "import mutual_info as mi\n",
    "import time\n",
    "from scipy.stats import wasserstein_distance\n",
    "import sys\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conceptnet_lite\n",
    "conceptnet_lite.connect(\"../OPENAI/data/conceptnet.db\")\n",
    "from conceptnet_lite import Label, edges_for, edges_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_distancia(texto_v,hipotesis_v,texto_t,texto_h,b_col,b_index):\n",
    "    lista_l=[]\n",
    "    for i in range(len(texto_t)):\n",
    "        lista=[]\n",
    "        for j in range(len(texto_h)):\n",
    "            lista.append(np.linalg.norm(texto_v[i] - hipotesis_v[j]))#*wasserstein_distance(texto_2[i],hipotesis_2[j]))\n",
    "        lista_l.append(lista)\n",
    "    df_distEuc=pd.DataFrame(lista_l,index=texto_t,columns=texto_h)\n",
    "    df_distEuc=df_distEuc.drop(b_col[1:],axis=1)\n",
    "    df_distEuc=df_distEuc.drop(b_index[1:],axis=0)\n",
    "    return df_distEuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_mutual_inf(texto_v,hipotesis_v,texto_t,texto_h):  \n",
    "    lista_l=[]\n",
    "    lista_muinfor=[]   \n",
    "    for i in range(len(texto_t)):\n",
    "        lista=[]\n",
    "        lista_mu=[]\n",
    "        for j in range(len(texto_h)):\n",
    "            lista.append(wasserstein_distance(texto_v[i],hipotesis_v[j]))\n",
    "            lista_mu.append(mi.mutual_information_2d(np.array(texto_v[i]),np.array(hipotesis_v[j])))\n",
    "        lista_l.append(lista)\n",
    "        lista_muinfor.append(lista_mu)\n",
    "    DFmearth=pd.DataFrame(lista_l,index=texto_t,columns=texto_h)\n",
    "    DFmutual_inf=pd.DataFrame(lista_muinfor,index=texto_t,columns=texto_h)\n",
    "    return DFmearth,DFmutual_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia(X):\n",
    "    \"\"\"Devuelve el valor de entropia de una muestra de datos\"\"\" \n",
    "    probs = [np.mean(X == valor) for valor in set(X)]\n",
    "    return round(sum(-p * np.log2(p) for p in probs), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "relaciones_generales=[\"related_to\",\"is_a\",\"etymologically_related_to\",\"manner_of\",\"has_a\",\"derived_from\",\"has_property\",\"form_of\",\"causes\",\"has_prerequisite\",\"has_subevent\",\"has_first_subevent\"]\n",
    "relaciones_especificas=[\"is_a\",\"manner_of\",\"has_a\",\"derived_from\",\"has_property\",\"form_of\",\"causes\",\"has_prerequisite\",\"has_subevent\",\"has_first_subevent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_synonyms(word):\n",
    "    sinonimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name == \"synonym\":\n",
    "                if word== e.start.text:\n",
    "                    sinonimos.add(e.end.text)\n",
    "                elif word== e.end.text:\n",
    "                    sinonimos.add(e.start.text)\n",
    "    except:\n",
    "        pass\n",
    "    sinonimos.add(word)\n",
    "    return sinonimos\n",
    "\n",
    "def bag_of_antonyms(word):\n",
    "    antonimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name in [\"antonym\",\"distinc_from\"]:\n",
    "                if word== e.start.text:\n",
    "                    antonimos.add(e.end.text)\n",
    "                elif word== e.end.text:\n",
    "                    antonimos.add(e.start.text)\n",
    "    except:\n",
    "        pass\n",
    "    return antonimos\n",
    "\n",
    "def bag_of_hyperonyms(word):\n",
    "    hiperonimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name in relaciones_generales:\n",
    "                if word== e.start.text:\n",
    "                    hiperonimos.add(e.end.text)\n",
    "    except:\n",
    "        pass\n",
    "    return hiperonimos\n",
    "\n",
    "def bag_of_hyponyms(word):\n",
    "    hiponimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name in relaciones_especificas:\n",
    "                if word== e.end.text:\n",
    "                    hiponimos.add(e.start.text)\n",
    "                    #print(e.relation.name,e.start.text)\n",
    "    except:\n",
    "        pass\n",
    "    return hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_distanceDavid2(s1, s2,sinT,sinH,HipT,hipH) :\n",
    "    bandera=True\n",
    "\n",
    "    # Length of two strings\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "\n",
    "    # If the listas de tokens are equal \n",
    "    if len1==len2:\n",
    "        for i in range(len1):\n",
    "            if s1[i]!=s2[i]:\n",
    "                bandera=False\n",
    "                break\n",
    "        if (bandera):\n",
    "            return 1.0; \n",
    " \n",
    "    if (len1 == 0 or len2 == 0) :\n",
    "        return 0.0; \n",
    " \n",
    "    # Maximum distance upto which matching \n",
    "    # is allowed \n",
    "    max_dist = (max(len(s1), len(s2)) // 2 )-1 ; \n",
    " \n",
    "    # Count of matches \n",
    "    match = 0; \n",
    "    matchC = 0; \n",
    " \n",
    "    # Hash for matches \n",
    "    hash_s1 = [0] * len(s1)\n",
    "    hash_s2 = [0] * len(s2)\n",
    " \n",
    "    # Traverse through the first string \n",
    "    for j in range( max(0, i - max_dist),\n",
    "                    min(len2, i + max_dist + 1)) : \n",
    "            print(s1[i],s2[j])\n",
    "            # If there is a match or is contain in a bag of sinomys of tk\n",
    "            if ((s1[i] == s2[j] or s1[i] in sinH[j] or s2[j] in sinT[i]) and hash_s2[j] == 0) : \n",
    "                print(s1[i],s2[j],\"sinonimos\")\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s1[i] in hipH[j] or len((sinT[i]).intersection(hipH[j]))>0) and hash_s2[j] == 0):\n",
    "                print(\"hiperonimos\",s2[j],s1[i],(sinT[i]).intersection(hipH[j]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s2[j] in HipT[i] or len((sinH[j]).intersection(HipT[i]))>0) and hash_s2[j] == 0): \n",
    "                print(\"hiperonimos sobre sinonimos\",s2[j],s1[i])\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif len((hipH[j]).intersection(HipT[i]))>0 and hash_s2[j] == 0: \n",
    "                print(\"hiph HiperT\",s2[j],s1[i],hipH[j],HipT[i],(hipH[j]).intersection(HipT[i]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "    print(hash_s1)\n",
    "    print(hash_s2)\n",
    "    print(match)\n",
    "    # If there is no match \n",
    "    if (match == 0) :\n",
    "        return 0.0; \n",
    "    return match/len2 ,(len2-match-matchC)/len2; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_distance(s1, s2,sinT,sinH,HipT,hipH) :\n",
    "    bandera=True\n",
    "\n",
    "    # Length of two strings\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "\n",
    "    # If the listas de tokens are equal \n",
    "    if len1==len2:\n",
    "        for i in range(len1):\n",
    "            if s1[i]!=s2[i]:\n",
    "                bandera=False\n",
    "                break\n",
    "        if (bandera):\n",
    "            return 1.0; \n",
    " \n",
    "    if (len1 == 0 or len2 == 0) :\n",
    "        return 0.0; \n",
    " \n",
    "    # Maximum distance upto which matching \n",
    "    # is allowed \n",
    "    max_dist = (max(len(s1), len(s2)) // 2 ); \n",
    " \n",
    "    # Count of matches \n",
    "    match = 0; \n",
    " \n",
    "    # Hash for matches \n",
    "    hash_s1 = [0] * len(s1)\n",
    "    hash_s2 = [0] * len(s2)\n",
    " \n",
    "    # Traverse through the first string \n",
    "    for i in range(len1):\n",
    " \n",
    "        # Check if there is any matches\n",
    "        for j in range(max(0, i - max_dist), \n",
    "                       min(len2, i + max_dist + 1)):\n",
    "            print(s1[i],s2[j])\n",
    "            # If there is a match or is contain in a bag of sinomys of tk\n",
    "            if ((s1[i] == s2[j] or s1[i] in sinH[j] or s2[j] in sinT[i]) and hash_s2[j] == 0) : \n",
    "                print(s1[i],s2[j],\"sinonimos\")\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s1[i] in hipH[j] or len((sinT[i]).intersection(hipH[j]))>0) and hash_s2[j] == 0):\n",
    "                print(\"hiponimos\",s2[j],s1[i],(sinT[i]).intersection(hipH[j]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s2[j] in HipT[i] or len((sinH[j]).intersection(HipT[i]))>0) and hash_s2[j] == 0): \n",
    "                print(\"hiperonimos sobre sinonimos\",s2[j],s1[i])\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif len((hipH[j]).intersection(HipT[i]))>0 and hash_s2[j] == 0: \n",
    "                print(\"hiperonimos3\",s2[j],s1[i],(hipH[j]).intersection(HipT[i]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            \n",
    "    print(hash_s1)\n",
    "    print(hash_s2)\n",
    "    print(match)\n",
    "    # If there is no match \n",
    "    if (match == 0) :\n",
    "        return 0.0; \n",
    " \n",
    "    # Number of transpositions \n",
    "    t = 0; \n",
    " \n",
    "    point = 0; \n",
    " \n",
    "    # Count number of occurrences \n",
    "    # where two characters match but \n",
    "    # there is a third matched character \n",
    "    # in between the indices \n",
    "    for i in range(len1) : \n",
    "        if (hash_s1[i]) :\n",
    "            # Find the next matched character \n",
    "            # in second string \n",
    "            while (hash_s2[point] == 0) :\n",
    "                point += 1; \n",
    " \n",
    "            if (s1[i] != s2[point]) :\n",
    "                point += 1\n",
    "                t += 1\n",
    "            else :\n",
    "                point += 1    \n",
    "    t /= 2; \n",
    "    #Return the Jaro Similarity \n",
    "    return (match/ len1 + match / len2 +\n",
    "            (match - t) / match)/ 3.0; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\") # modelo de nlp\n",
    "\n",
    "#ut.load_vectors_in_lang(nlp,\"../OPENAI/data/glove.840B.300d.txt\") # carga de vectores en nlp.wv\n",
    "ut.load_vectors_in_lang(nlp,\"./data/numberbatch-en-17.04b.txt\") # carga de vectores en nlp.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#prueba=pd.read_csv(\"data/DEV/pruebaDEV.csv\")\n",
    "#prueba=pd.read_csv(\"../OPENAI/data/\"+sys.argv[1])\n",
    "\n",
    "#textos = prueba[\"sentence1\"].to_list()       # almacenamiento en listas\n",
    "#hipotesis = prueba[\"sentence2\"].to_list()\n",
    "textos=[\"A man dressed in a red shirt and black tie stands up at a wedding reception to make a speech.\",\"Fishermen using poison sodium cyanide have helped destroy estimated 70 reefs\",\"A woman finishing a marathon race.\",\"Zapatero visited the following cities in four days: Brasilia, São Paulo, Buenos Aires and Santiago de Chile. According to official sources these visits are the last part of the project he began at the EU-Latin American Summit in Guadalajara, Mexico and pursued in the Ibero-American meeting in Costa Rica in November.\",\"The child is hanging upside down with his legs over a pole.\",\"Someone is falling off a horse\",\"As late as 1799, priests were still being imprisoned or deported to penal colonies and persecution only worsened after the French army led by General Louis Alexandre Berthier captured Rome and imprisoned Pope Pius VI, who would die in captivity in Valence, Drôme, France in August of 1799.\"]\n",
    "hipotesis= [\"a guy in a red top and tie makes a speech\",\"Cyanide fishing linked destruction area reefs\",\"The woman is not going to finish the race.\",\"Zapatero participated in the Ibero-American meeting in Costa Rica.\",\"There is an adult upside down.\",\"The man is not knocked off of a horse\",\"Alexandre Berthier died in 1799.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[ 'acl','acomp','advcl','advmod','amod','appos','ccomp','complm','compound','conj','infmod','meta','neg',\n",
    " 'nmod','nn','npadvmod','nounmod','npmod','num','number','nummod','partmod','pcomp','poss','possessive',\n",
    " #'prep',\n",
    " 'quantmod', 'rcmod', 'relcl', 'xcomp', 'adc', 'avc', 'mnr', 'mo', 'ng', 'nmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion_entidades(nlp,texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    dir_sust=dict()\n",
    "    palabras=[]\n",
    "    for token in doc:\n",
    "        if token.dep_ in tags or \"mod\" in token.dep_ or \"comp\" in token.dep_ or \"neg\" in token.dep_:\n",
    "            #print(token.text, token.lemma_, token.pos_,token.dep_,token.head.text,token.head.lemma_, token.head.pos_,\n",
    "            #    [child for child in token.children])\n",
    "            #sustantivos.append(token.head.lemma_)\n",
    "            if token.head.lemma_ in dir_sust:\n",
    "                if dir_sust[token.head.lemma_]==\"NA\":\n",
    "                    dir_sust[token.head.lemma_]=token.lemma_\n",
    "                    if token.head not in palabras:\n",
    "                        palabras.append(token.head)\n",
    "                else:\n",
    "                    dir_sust[token.head.lemma_]=dir_sust[token.head.lemma_]+\",\"+token.lemma_\n",
    "                    if token.head not in palabras:\n",
    "                        palabras.append(token.head)\n",
    "            else:\n",
    "                dir_sust[token.head.lemma_]=token.lemma_\n",
    "                if token.head not in palabras:\n",
    "                    palabras.append(token.head)\n",
    "        elif token.pos_ in [\"NOUN\"]:\n",
    "            #print(token.text,token.lemma_, token.pos_,token.dep_)\n",
    "            #sustantivos.append(token.lemma_)\n",
    "            if token.lemma_ not in dir_sust:\n",
    "                dir_sust[token.lemma_]=\"NA\"\n",
    "                if token not in palabras:\n",
    "                    palabras.append(token)\n",
    "        elif token.pos_ in [\"VERB\"]:\n",
    "            #print(token.text, token.lemma_,token.pos_,token.dep_)\n",
    "            #sustantivos.append(token.lemma_)\n",
    "            if token.lemma_ not in dir_sust:\n",
    "                dir_sust[token.lemma_]=\"NA\"\n",
    "                if token not in palabras:\n",
    "                    palabras.append(token)\n",
    "    return dir_sust,palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'man': 'NA', 'knock': 'not', 'horse': 'NA'}, [man, knocked, horse])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representacion_entidades(nlp,\"There is an adult upside down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion2(nlp,texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    dir_sust=dict()\n",
    "    palabras=[]\n",
    "    noun_phrase= [chunk.lemma_ for chunk in doc.noun_chunks]\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    for chunk in noun_phrase:\n",
    "        doc = nlp(chunk)\n",
    "        nue=[]\n",
    "        for token in doc:\n",
    "            if token.pos_!=\"DET\":\n",
    "                nue.append(token.lemma_)\n",
    "        if len(nue)>1:\n",
    "            dir_sust[nue[-1]]=','.join(nue[:-1])\n",
    "        else:\n",
    "            dir_sust[nue[-1]]=\"NA\"\n",
    "    # for chunk in doc.noun_chunks:\n",
    "    #     if chunk.root.lemma_!=chunk.lemma_:\n",
    "    #         dir_sust[chunk.root.lemma_]=','.join(chunk.lemma_.split()[:-1])\n",
    "    #     else:\n",
    "    #         dir_sust[chunk.root.lemma_]=\"NA\"\n",
    "    for v in verbs:\n",
    "        dir_sust[v]=\"NA\"\n",
    "    #representacion_entidades(nlp,texto)\n",
    "    palabras.extend([chunk.root.lemma_ for chunk in doc.noun_chunks])\n",
    "    palabras.extend(verbs)\n",
    "    return dir_sust,palabras\n",
    "\n",
    "def relacion_entailment(wt,wh):\n",
    "    try:\n",
    "        concepts_wt = Label.get(text=wt, language='en').concepts\n",
    "        concepts_wh = Label.get(text=wh, language='en').concepts\n",
    "        for e in edges_between(concepts_wt, concepts_wh):\n",
    "            if wt == e.start.text and e.relation.name in relaciones_generales:\n",
    "                print(e.start.text, \"-\", e.end.text, \"|\", e.relation.name,e)\n",
    "                return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negacion(nlp,texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    for token in doc:\n",
    "        if(token.dep_==\"neg\"):\n",
    "            return True, token.head.lemma_\n",
    "    return False,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relacion_noentailment(wt,wh):\n",
    "    try:\n",
    "        concepts_wt = Label.get(text=wt, language='en').concepts\n",
    "        concepts_wh = Label.get(text=wh, language='en').concepts\n",
    "        for e in edges_between(concepts_wt, concepts_wh):\n",
    "            if wt == e.start.text and e.relation.name in [\"distinct_from\",\"antonym\"]:\n",
    "                print(e.start.text, \"-\", e.end.text, \"|\", e.relation.name,e)\n",
    "                return True\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'sumas' : [], 'distancias' : [], 'entropia_total' : [],'entropias' : [],'mutinf' : [], \n",
    "            'mearts' : [], 'max_info' : [],  'list_comp' : [], 'diferencias' :[], 'list_incomp':[],\n",
    "            'list_M' : [], 'list_m' : [], 'list_T' : [], 'Jaro-Winkler_rit':[],\n",
    "            'negT' : [], 'verbT' : [], 'begH' : [], 'verbH':[], \n",
    "            'clases' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "A man dressed in a red shirt and black tie stands up at a wedding reception to make a speech.\n",
      "texto man dress\n",
      "texto shirt red,tie\n",
      "texto tie black\n",
      "texto stand make\n",
      "texto reception wedding\n",
      "texto speech NA\n",
      "a guy in a red top and tie makes a speech\n",
      "hipotesis guy tie\n",
      "hipotesis top red\n",
      "hipotesis make NA\n",
      "hipotesis speech NA\n",
      "clean\n",
      "['man', 'shirt', 'tie', 'stand', 'reception', 'speech']\n",
      "['guy', 'top', 'make', 'speech']\n",
      "['man', 'dress', 'red', 'shirt', 'black', 'tie', 'stand', 'wedding', 'reception', 'make', 'speech']\n",
      "['guy', 'red', 'top', 'tie', 'make', 'speech']\n",
      "man guy\n",
      "hiperonimos sobre sinonimos guy man\n",
      "dress guy\n",
      "dress red\n",
      "hiperonimos sobre sinonimos red dress\n",
      "red guy\n",
      "red red\n",
      "red top\n",
      "hiperonimos3 top red {'stop'}\n",
      "shirt guy\n",
      "shirt red\n",
      "shirt top\n",
      "shirt tie\n",
      "hiperonimos sobre sinonimos tie shirt\n",
      "black guy\n",
      "black red\n",
      "black top\n",
      "black tie\n",
      "black make\n",
      "black speech\n",
      "tie guy\n",
      "tie red\n",
      "tie top\n",
      "tie tie\n",
      "tie make\n",
      "hiperonimos sobre sinonimos make tie\n",
      "stand red\n",
      "stand top\n",
      "stand tie\n",
      "stand make\n",
      "stand speech\n",
      "wedding top\n",
      "wedding tie\n",
      "wedding make\n",
      "wedding speech\n",
      "reception tie\n",
      "reception make\n",
      "reception speech\n",
      "make make\n",
      "make speech\n",
      "speech speech\n",
      "speech speech sinonimos\n",
      "[1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "6\n",
      "['dress']\n",
      "['tie']\n",
      "man - guy | related_to /a/[/r/related_to/,/c/en/man/,/c/en/guy/]\n",
      "man guy True\n",
      "checar atributos\n",
      "lo que tiene: guy tie\n",
      "tie dress\n",
      "['red', 'tie']\n",
      "['red']\n",
      "shirt - top | is_a /a/[/r/is_a/,/c/en/shirt/n/,/c/en/top/n/opencyc/clothing_top/]\n",
      "shirt top True\n",
      "checar atributos\n",
      "lo que tiene: top red\n",
      "red red\n",
      "red red True\n",
      "red tie\n",
      "['make']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "stand make Hiperonimos y sinonimos {'make', 'produce', 'draw', 'take', 'work', 'hold', 'have', 'gain', 'drive', 'origin'}\n",
      "checar atributos\n",
      "speech speech True misma palabra\n",
      "                guy\n",
      "man        0.616642\n",
      "shirt      0.095947\n",
      "tie        0.054360\n",
      "stand      0.118021\n",
      "reception  0.021067\n",
      "speech     0.011122\n",
      "1\n",
      "Fishermen using poison sodium cyanide have helped destroy estimated 70 reefs\n",
      "texto fisherman use\n",
      "texto sodium poison\n",
      "texto cyanide sodium\n",
      "texto help destroy\n",
      "texto reef estimate,70\n",
      "Cyanide fishing linked destruction area reefs\n",
      "hipotesis fishing cyanide\n",
      "hipotesis link NA\n",
      "hipotesis area destruction\n",
      "hipotesis reef area\n",
      "clean\n",
      "['fisherman', 'sodium', 'cyanide', 'help', 'reef']\n",
      "['fishing', 'link', 'area', 'reef']\n",
      "['fisherman', 'use', 'poison', 'sodium', 'cyanide', 'help', 'destroy', 'estimate', '70', 'reef']\n",
      "['cyanide', 'fishing', 'link', 'destruction', 'area', 'reef']\n",
      "fisherman cyanide\n",
      "fisherman fishing\n",
      "hiperonimos sobre sinonimos fishing fisherman\n",
      "use cyanide\n",
      "use fishing\n",
      "use link\n",
      "use destruction\n",
      "use area\n",
      "use reef\n",
      "poison cyanide\n",
      "hiperonimos sobre sinonimos cyanide poison\n",
      "sodium cyanide\n",
      "sodium fishing\n",
      "sodium link\n",
      "sodium destruction\n",
      "sodium area\n",
      "sodium reef\n",
      "cyanide cyanide\n",
      "cyanide fishing\n",
      "cyanide link\n",
      "cyanide destruction\n",
      "cyanide area\n",
      "cyanide reef\n",
      "help cyanide\n",
      "help fishing\n",
      "help link\n",
      "help destruction\n",
      "help area\n",
      "help reef\n",
      "destroy fishing\n",
      "destroy link\n",
      "destroy destruction\n",
      "hiponimos destruction destroy {'ravage', 'kill', 'ruin'}\n",
      "estimate link\n",
      "estimate destruction\n",
      "estimate area\n",
      "estimate reef\n",
      "70 destruction\n",
      "70 area\n",
      "70 reef\n",
      "reef area\n",
      "reef reef\n",
      "reef reef sinonimos\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[1, 1, 0, 1, 0, 1]\n",
      "4\n",
      "['use']\n",
      "['cyanide']\n",
      "fisherman - fishing | related_to /a/[/r/related_to/,/c/en/fisherman/n/,/c/en/fishing/]\n",
      "fisherman fishing True\n",
      "checar atributos\n",
      "lo que tiene: fishing cyanide\n",
      "cyanide use\n",
      "['destroy']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "['estimate', '70']\n",
      "['destruction']\n",
      "Proceso de conjuntos\n",
      "reef reef True misma palabra\n",
      "            fishing      link      area      reef\n",
      "fisherman  0.785934 -0.049944  0.023814  0.355217\n",
      "sodium     0.015444 -0.022095 -0.054695  0.108704\n",
      "cyanide    0.026225  0.012227 -0.014991  0.036029\n",
      "help      -0.042190  0.113030  0.052974 -0.006249\n",
      "reef       0.358626  0.012770  0.133126  1.000000\n",
      "2\n",
      "A woman finishing a marathon race.\n",
      "texto woman NA\n",
      "texto finish NA\n",
      "texto race marathon\n",
      "The woman is not going to finish the race.\n",
      "hipotesis woman NA\n",
      "hipotesis go not,finish\n",
      "hipotesis race NA\n",
      "clean\n",
      "['woman', 'finish', 'race']\n",
      "['woman', 'go', 'race']\n",
      "['woman', 'finish', 'marathon', 'race']\n",
      "['woman', 'not', 'go', 'finish', 'race']\n",
      "woman woman\n",
      "woman woman sinonimos\n",
      "finish woman\n",
      "finish not\n",
      "finish go\n",
      "finish finish\n",
      "finish finish sinonimos\n",
      "marathon woman\n",
      "marathon not\n",
      "marathon go\n",
      "marathon finish\n",
      "marathon race\n",
      "hiponimos race marathon {'marathon'}\n",
      "race not\n",
      "hiperonimos3 not race {'people'}\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 1, 1]\n",
      "4\n",
      "woman woman True misma palabra\n",
      "['NA']\n",
      "['not', 'finish']\n",
      "Proceso de conjuntos\n",
      "finish go Hiperonimos y sinonimos {'journey', 'function', 'attempt', 'fare', 'travel', 'tour', 'proceed'}\n",
      "checar atributos\n",
      "lo que tiene: go not,finish\n",
      "race race True misma palabra\n",
      "              go\n",
      "woman  -0.024095\n",
      "finish  0.255300\n",
      "race    0.081907\n",
      "3\n",
      "Zapatero visited the following cities in four days: Brasilia, São Paulo, Buenos Aires and Santiago de Chile. According to official sources these visits are the last part of the project he began at the EU-Latin American Summit in Guadalajara, Mexico and pursued in the Ibero-American meeting in Costa Rica in November.\n",
      "texto air visit,chile\n",
      "texto city follow\n",
      "texto day four\n",
      "texto paulo são\n",
      "texto brasilia paulo\n",
      "texto chile santiago,de\n",
      "texto accord NA\n",
      "texto source official\n",
      "texto visit NA\n",
      "texto part last\n",
      "texto project begin\n",
      "texto summit eu,american\n",
      "texto american latin,ibero\n",
      "texto guadalajara mexico\n",
      "texto begin pursue\n",
      "texto meeting american\n",
      "texto rica costa\n",
      "Zapatero participated in the Ibero-American meeting in Costa Rica.\n",
      "hipotesis participate NA\n",
      "hipotesis american ibero\n",
      "hipotesis meeting american\n",
      "hipotesis rica costa\n",
      "clean\n",
      "['air', 'city', 'day', 'paulo', 'brasilia', 'chile', 'accord', 'source', 'visit', 'part', 'project', 'summit', 'american', 'guadalajara', 'begin', 'meeting', 'rica']\n",
      "['participate', 'american', 'meeting', 'rica']\n",
      "['zapatero', 'visit', 'follow', 'city', 'four', 'day', 'brasilia', 'são', 'paulo', 'buenos', 'air', 'santiago', 'chile', 'accord', 'official', 'source', 'visit', 'last', 'part', 'project', 'begin', 'eu', 'latin', 'american', 'summit', 'guadalajara', 'mexico', 'pursue', 'ibero', 'american', 'meeting', 'costa', 'rica', 'november']\n",
      "['zapatero', 'participate', 'ibero', 'american', 'meeting', 'costa', 'rica']\n",
      "zapatero zapatero\n",
      "zapatero zapatero sinonimos\n",
      "visit zapatero\n",
      "visit participate\n",
      "visit ibero\n",
      "visit american\n",
      "visit meeting\n",
      "hiponimos meeting visit {'visit'}\n",
      "follow zapatero\n",
      "follow participate\n",
      "follow ibero\n",
      "follow american\n",
      "follow meeting\n",
      "follow costa\n",
      "follow rica\n",
      "city zapatero\n",
      "city participate\n",
      "city ibero\n",
      "city american\n",
      "city meeting\n",
      "city costa\n",
      "city rica\n",
      "four zapatero\n",
      "four participate\n",
      "four ibero\n",
      "four american\n",
      "four meeting\n",
      "four costa\n",
      "four rica\n",
      "day zapatero\n",
      "day participate\n",
      "day ibero\n",
      "day american\n",
      "day meeting\n",
      "day costa\n",
      "day rica\n",
      "brasilia zapatero\n",
      "brasilia participate\n",
      "brasilia ibero\n",
      "brasilia american\n",
      "brasilia meeting\n",
      "brasilia costa\n",
      "brasilia rica\n",
      "são zapatero\n",
      "são participate\n",
      "são ibero\n",
      "são american\n",
      "são meeting\n",
      "são costa\n",
      "são rica\n",
      "paulo zapatero\n",
      "paulo participate\n",
      "paulo ibero\n",
      "paulo american\n",
      "paulo meeting\n",
      "paulo costa\n",
      "paulo rica\n",
      "buenos zapatero\n",
      "buenos participate\n",
      "buenos ibero\n",
      "buenos american\n",
      "buenos meeting\n",
      "buenos costa\n",
      "buenos rica\n",
      "air zapatero\n",
      "air participate\n",
      "air ibero\n",
      "air american\n",
      "air meeting\n",
      "air costa\n",
      "air rica\n",
      "santiago zapatero\n",
      "santiago participate\n",
      "santiago ibero\n",
      "santiago american\n",
      "santiago meeting\n",
      "santiago costa\n",
      "santiago rica\n",
      "chile zapatero\n",
      "chile participate\n",
      "chile ibero\n",
      "chile american\n",
      "chile meeting\n",
      "chile costa\n",
      "chile rica\n",
      "accord zapatero\n",
      "accord participate\n",
      "accord ibero\n",
      "accord american\n",
      "accord meeting\n",
      "accord costa\n",
      "accord rica\n",
      "official zapatero\n",
      "official participate\n",
      "official ibero\n",
      "official american\n",
      "official meeting\n",
      "official costa\n",
      "official rica\n",
      "source zapatero\n",
      "source participate\n",
      "source ibero\n",
      "source american\n",
      "source meeting\n",
      "source costa\n",
      "source rica\n",
      "visit zapatero\n",
      "visit participate\n",
      "visit ibero\n",
      "visit american\n",
      "visit meeting\n",
      "visit costa\n",
      "visit rica\n",
      "last zapatero\n",
      "last participate\n",
      "last ibero\n",
      "last american\n",
      "last meeting\n",
      "last costa\n",
      "last rica\n",
      "part participate\n",
      "part ibero\n",
      "part american\n",
      "part meeting\n",
      "part costa\n",
      "part rica\n",
      "project ibero\n",
      "project american\n",
      "project meeting\n",
      "project costa\n",
      "project rica\n",
      "begin american\n",
      "begin meeting\n",
      "begin costa\n",
      "begin rica\n",
      "eu meeting\n",
      "eu costa\n",
      "eu rica\n",
      "latin costa\n",
      "latin rica\n",
      "american rica\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 1, 0, 0]\n",
      "2\n",
      "['last']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "part participate Hiperonimos y sinonimos {'partake', 'participate'}\n",
      "checar atributos\n",
      "american american True misma palabra\n",
      "ibero ibero la misma\n",
      "meeting meeting True misma palabra\n",
      "american - american | is_a /a/[/r/is_a/,/c/en/american/n/,/c/en/american/n/]\n",
      "american - american | is_a /a/[/r/is_a/,/c/en/american/n/,/c/en/american/n/]\n",
      "american american True\n",
      "rica rica True misma palabra\n",
      "costa costa la misma\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [air, city, day, paulo, brasilia, chile, accord, source, visit, part, project, summit, american, guadalajara, begin, meeting, rica]\n",
      "4\n",
      "The child is hanging upside down with his legs over a pole.\n",
      "texto child NA\n",
      "texto hang down\n",
      "texto down upside\n",
      "texto leg his\n",
      "texto pole NA\n",
      "There is an adult upside down.\n",
      "hipotesis be down\n",
      "hipotesis upside adult\n",
      "hipotesis down upside\n",
      "clean\n",
      "['child', 'hang', 'down', 'leg', 'pole']\n",
      "['be', 'upside', 'down']\n",
      "['child', 'hang', 'upside', 'down', 'leg', 'pole']\n",
      "['adult', 'upside', 'down']\n",
      "child adult\n",
      "hiperonimos sobre sinonimos adult child\n",
      "hang adult\n",
      "hang upside\n",
      "hiperonimos sobre sinonimos upside hang\n",
      "upside adult\n",
      "upside upside\n",
      "upside down\n",
      "down adult\n",
      "down upside\n",
      "down down\n",
      "down down sinonimos\n",
      "leg upside\n",
      "leg down\n",
      "pole down\n",
      "[1, 1, 0, 1, 0, 0]\n",
      "[1, 1, 1]\n",
      "3\n",
      "['down']\n",
      "['down']\n",
      "hang - be | manner_of /a/[/r/manner_of/,/c/en/hang/v/wn/stative/,/c/en/be/v/wn/stative/]\n",
      "hang be True\n",
      "checar atributos\n",
      "lo que tiene: be down\n",
      "down down\n",
      "down down True\n",
      "['upside']\n",
      "['adult']\n",
      "Proceso de conjuntos\n",
      "down down True misma palabra\n",
      "upside upside la misma\n",
      "         upside\n",
      "child -0.018140\n",
      "hang   0.232282\n",
      "down   0.430902\n",
      "leg    0.053865\n",
      "pole   0.166064\n",
      "5\n",
      "Someone is falling off a horse\n",
      "texto fall NA\n",
      "texto horse NA\n",
      "The man is not knocked off of a horse\n",
      "hipotesis man NA\n",
      "hipotesis knock not\n",
      "hipotesis horse NA\n",
      "clean\n",
      "['fall', 'horse']\n",
      "['man', 'knock', 'horse']\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "for i in range(len(textos)):\n",
    "    print(i)\n",
    "    print(textos[i])\n",
    "    #r_t,t_clean_m=representacion_entidades(nlp,textos[i])\n",
    "    r_t,t_clean_m=representacion2(nlp,textos[i])\n",
    "    neg_t,negadat=negacion(nlp,textos[i])\n",
    "    new_data['negT'].append(neg_t)\n",
    "    new_data['verbT'].append(negadat)\n",
    "    #print(list(r_t.keys()))\n",
    "    for clave in r_t.keys():\n",
    "        print(\"texto\",clave,r_t[clave])\n",
    "    print(hipotesis[i])\n",
    "    #r_h,h_clean_m = representacion_entidades(nlp,hipotesis[i])\n",
    "    r_h,h_clean_m = representacion2(nlp,hipotesis[i])\n",
    "    neg_h,negadah=negacion(nlp,hipotesis[i])\n",
    "    new_data['negH'].append(neg_h)\n",
    "    new_data['verbH'].append(negadah)\n",
    "    for clave in r_h.keys():\n",
    "        print(\"hipotesis\",clave,r_h[clave])\n",
    "        \n",
    "    #print(list(r_h.keys()))\n",
    "    # t_clean=' '.join(list(r_t.keys()))\n",
    "    # h_clean=' '.join(list(r_h.keys()))\n",
    "    t_clean=list(r_t.keys())\n",
    "    h_clean=list(r_h.keys())\n",
    "    print(\"clean\")\n",
    "    print(t_clean)\n",
    "    print(h_clean)\n",
    "    # t_clean_m=ut.get_words(textos[i],nlp,pos_to_remove=['PUNCT'], normed=True,lemmatize=False)\n",
    "    # h_clean_m=ut.get_words(hipotesis[i],nlp,pos_to_remove=['PUNCT'], normed=True,lemmatize=False)\n",
    "    # print(\"get_words\")\n",
    "    # print(\"clean_m\")\n",
    "    # print(t_clean_m)\n",
    "    # print(h_clean_m)\n",
    "    # t_clean_m=ut.reform_sentence2(t_clean_m,nlp)\n",
    "    # h_clean_m=ut.reform_sentence2(h_clean_m,nlp)\n",
    "    # print(\"reform2\")\n",
    "    # print(t_clean_m)\n",
    "    # print(h_clean_m)\n",
    "    # t_clean=ut.get_words_rep(t_clean_m,nlp,pos_to_remove=[\"\"],normed=True,lemmatize=False)\n",
    "    # h_clean=ut.get_words_rep(h_clean_m,nlp,pos_to_remove=[\"\"],normed=True,lemmatize=False)\n",
    "    # print(\"t_clean\")\n",
    "    # print(t_clean)\n",
    "    # print(h_clean)\n",
    "    # print(\"clean_m\")\n",
    "    # print(t_clean_m)\n",
    "    # print(h_clean_m)\n",
    "    t_vectors=ut.get_matrix_rep2(t_clean, nlp, normed=False)\n",
    "    h_vectors=ut.get_matrix_rep2(h_clean, nlp, normed=False)\n",
    "    t_vectors_n=ut.get_matrix_rep2(t_clean, nlp, normed=True)\n",
    "    h_vectors_n=ut.get_matrix_rep2(h_clean, nlp, normed=True)\n",
    "    \n",
    "    # print(t_vectors)\n",
    "    # print(h_vectors)\n",
    "    # print(t_clean_m,h_clean_m)\n",
    "    \n",
    "    # s1=t_clean_m.split()\n",
    "    # s2=h_clean_m.split()\n",
    "    t_lem=ut.get_lemmas_(textos[i],nlp)\n",
    "    h_lem=ut.get_lemmas_(hipotesis[i],nlp)\n",
    "    s1=t_lem\n",
    "    s2=h_lem\n",
    "    \n",
    "    sinT=[]\n",
    "    antT=[]\n",
    "    HipT=[]\n",
    "    sinH=[]\n",
    "    antH=[]\n",
    "    HipH=[]\n",
    "    hipH=[]\n",
    "    \n",
    "    # # encontrar bolsa de sinonimos de cada token\n",
    "    for t in s1:\n",
    "        sinT.append(bag_of_synonyms(t))\n",
    "        HipT.append(bag_of_hyperonyms(t))\n",
    "\n",
    "    for h in s2:\n",
    "        sinH.append(bag_of_synonyms(h))\n",
    "        hipH.append(bag_of_hyponyms(h))\n",
    "    print(t_lem)\n",
    "    print(h_lem)\n",
    "    tp1=jaro_distance(t_lem, h_lem,sinT,sinH,HipT,hipH)\n",
    "    new_data['Jaro-Winkler_rit'].append(tp1)\n",
    "    # #new_data['c_estructura'].append(tp2)\n",
    "    #new_data['nomatch'].append(tp3)\n",
    "    \n",
    "    #tp1,tp2=jaro_distance_relacionadas(t_lem, h_lem,sinT,HipT,sinH,hipH)\n",
    "    #new_data['Jaro-Winkler_relacionadas'].append(tp1)\n",
    "    #new_data['c2_estructura'].append(tp2)\n",
    "\n",
    "    #tp1,tp2=jaro_distance_contra(t_lem, h_lem,antT,HipT,antH,HipH)\n",
    "    #new_data['Jaro-Winkler_contra'].append(tp1)\n",
    "    #new_data['c1_estructura'].append(tp2)\n",
    "\n",
    "    # # Obtencion de matriz de alineamiento, matriz de move earth y mutual information\n",
    "    ma=np.dot(t_vectors_n,h_vectors_n.T)\n",
    "    #print(t_clean,h_clean)\n",
    "    #print(len(t_vectors_n),len(h_vectors_n),len(t_clean),len(h_clean))\n",
    "    m_earth,m_mi=wasserstein_mutual_inf(t_vectors_n,h_vectors_n,t_clean,h_clean)\n",
    "    ma=pd.DataFrame(ma,index=t_clean,columns=h_clean)\n",
    "    #print(ma)\n",
    "\n",
    "    # # Calculamos la entropia inicial de la matriz de distancias coseno sobre tokens de T y H\n",
    "    new_data['entropia_total'].append(entropia(ma.round(1).values.flatten())) \n",
    "\n",
    "    # ###### BORRADO DE COSAS QUE NO OCUPO, SOLO NOS QUEDAMOS CON INFORMACIÓN DE TIPOS DE PALABRA: NOUN, VERB, ADJ Y ADV\n",
    "    # # TAMBIÉN OMITIMOS EL VERBO BE DEBIDO A QUE POR LO REGULAR SE UTILIZA COMO AUXILIAR Y ES UN VERBO COPULATIVO\n",
    "    # # sirve para construir la llamada predicación nominal del sujeto de una oración: \n",
    "    # # #el sujeto se une con este verbo a un complemento obligatorio llamado atributo que por lo general determina \n",
    "    # # alguna propiedad, estado o equivalencia del mismo, por ejemplo: \"Este plato es bueno\". \"Juan está casado\".\n",
    "    c_compatibilidad=0\n",
    "    c_incompatibilidad=0\n",
    "    # c_rel_concep=0\n",
    "    b_col=[0]\n",
    "\n",
    "    new_data['list_T'].append(ma.shape[0])\n",
    "    new_data['list_M'].append(ma.shape[1])\n",
    "    \n",
    "    # # #PARA REVISAR SI EXISTEN RELACIONES DE SIMILITUD SEMÁNTICA A TRAVÉS DEL USO DE CONCEPNET\n",
    "    \n",
    "    n_index = ma.shape[0]\n",
    "    n_columns = ma.shape[1]\n",
    "    pasada=0\n",
    "    #print(ma.index,ma.columns)\n",
    "    combinaciones=[]\n",
    "\n",
    "    iguales=set(ma.index).intersection(set(ma.columns))\n",
    "\n",
    "    while n_columns>0 and pasada<4:\n",
    "        borrar=[]\n",
    "        a = ma.idxmax().values\n",
    "        b = ma.columns\n",
    "        for j in range(len(a)):\n",
    "            #print(a[j]+\"_\"+b[j] not in combinaciones,a[j]+\"_\"+b[j],combinaciones)\n",
    "            if a[j]+\"_\"+b[j] not in combinaciones:\n",
    "                combinaciones.append(a[j]+\"_\"+b[j])\n",
    "                if(relacion_noentailment(a[j],b[j])):\n",
    "                    c_incompatibilidad+=1\n",
    "                elif a[j]==b[j]: ## Si son palabras identicas entonces no hacemos más proceso\n",
    "                    print(a[j],b[j],True,\"misma palabra\")\n",
    "                    if 'NA'!=r_h[b[j]]:\n",
    "                        for mod_h in r_h[b[j]].split(\",\"):\n",
    "                            for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                if(relacion_entailment(mod_t,mod_h)):\n",
    "                                    print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                    borrar.append(b[j])\n",
    "                                    c_compatibilidad+=1\n",
    "                                elif(mod_t==mod_h):\n",
    "                                    print(mod_t,mod_h,\"la misma\")\n",
    "                                    borrar.append(b[j])\n",
    "                                    c_compatibilidad+=1\n",
    "                    else:\n",
    "                        borrar.append(b[j])\n",
    "                        c_compatibilidad+=1\n",
    "                else:# en otro caso buscamos una relación de generalidad\n",
    "                    print(r_t[a[j]].split(\",\"))\n",
    "                    print(r_h[b[j]].split(\",\"))\n",
    "                    r_e = relacion_entailment(a[j],b[j])\n",
    "                    if r_e:\n",
    "                        print(a[j],b[j], r_e)\n",
    "                        print(\"checar atributos\")\n",
    "                        if 'NA' !=r_h[b[j]]:\n",
    "                            print(\"lo que tiene:\",b[j],r_h[b[j]])\n",
    "                            for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                if 'NA'!=r_t[a[j]]:\n",
    "                                    for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                        print(mod_h,mod_t)\n",
    "                                        if mod_h==mod_t:\n",
    "                                            print(mod_t,mod_h,True)\n",
    "                                            borrar.append(b[j])\n",
    "                                            c_compatibilidad+=1\n",
    "                                        else:\n",
    "                                            if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                borrar.append(b[j])\n",
    "                                                c_compatibilidad+=1\n",
    "                        else:\n",
    "                            borrar.append(b[j])\n",
    "                            c_compatibilidad+=1\n",
    "                    else:\n",
    "                        print(\"Proceso de conjuntos\")\n",
    "                        sin1=bag_of_synonyms(a[j])\n",
    "                        sin2=bag_of_synonyms(b[j])\n",
    "                        if len(sin1.intersection(sin2))>0:\n",
    "                            print(a[j],b[j],\"conjuntos de sinonimos\",sin1.intersection(sin2))\n",
    "                            print(\"checar atributos\")\n",
    "                            if 'NA'!=r_h[b[j]]:\n",
    "                                print(\"lo que tiene:\",b[j],r_h[b[j]])\n",
    "                                for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                    if 'NA'!=r_t[a[j]]:\n",
    "                                        for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                            print(mod_h,mod_t)\n",
    "                                            if mod_h==mod_t:\n",
    "                                                print(mod_t,mod_h,True)\n",
    "                                                borrar.append(b[j])\n",
    "                                                c_compatibilidad+=1\n",
    "                                            else:\n",
    "                                                if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                    print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                    borrar.append(b[j])\n",
    "                                                    c_compatibilidad+=1\n",
    "                            else:\n",
    "                                borrar.append(b[j])\n",
    "                                c_compatibilidad+=1\n",
    "                        else:\n",
    "                            Hip1=set()\n",
    "                            for e in list(sin1):\n",
    "                                Hip1=Hip1.union(bag_of_hyperonyms(e))\n",
    "                            if len(Hip1.intersection(sin2))>0:\n",
    "                                print(a[j],b[j],\"Hiperonimos y sinonimos\",Hip1.intersection(sin2))\n",
    "                                print(\"checar atributos\")\n",
    "                                if 'NA'!=r_h[b[j]]:\n",
    "                                    print(\"lo que tiene:\",b[j],r_h[b[j]])\n",
    "                                    for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                        if 'NA'!=r_t[a[j]]:\n",
    "                                            for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                                print(mod_h,mod_t)\n",
    "                                                if mod_h==mod_t:\n",
    "                                                    print(mod_t,mod_h,True)\n",
    "                                                    borrar.append(b[j])\n",
    "                                                    c_compatibilidad+=1\n",
    "                                                else:\n",
    "                                                    if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                        print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                        borrar.append(b[j])\n",
    "                                                        c_compatibilidad+=1\n",
    "                                else:\n",
    "                                    borrar.append(b[j])\n",
    "                                    c_compatibilidad+=1\n",
    "                            else:\n",
    "                                hip2=set()\n",
    "                                for e in list(sin2):\n",
    "                                    hip2=hip2.union(bag_of_hyponyms(e))\n",
    "                                if len(sin1.intersection(hip2))>0:\n",
    "                                    print(a[j],b[j],\"Sinonimos e hiponimos\",sin1.intersection(hip2))\n",
    "                                    print(\"checar atributos\")\n",
    "                                    if 'NA'!=r_h[b[j]]:\n",
    "                                        for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                            if 'NA'!=r_t[a[j]]:\n",
    "                                                for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                                    #print(mod_h,mod_t)\n",
    "                                                    if mod_h==mod_t:\n",
    "                                                        #print(mod_t,mod_h,True)\n",
    "                                                        borrar.append(b[j])\n",
    "                                                        c_compatibilidad+=1\n",
    "                                                    else:\n",
    "                                                        if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                            #print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                            borrar.append(b[j])\n",
    "                                                            c_compatibilidad+=1\n",
    "                                    else:\n",
    "                                        borrar.append(b[j])\n",
    "                                        c_compatibilidad+=1\n",
    "            #else:\n",
    "                #print(a[j],b[j],\"ya se habia revisado\")\n",
    "        for equal in iguales:\n",
    "            if equal+\"_\"+equal not in combinaciones:\n",
    "                if 'NA'!=r_h[equal]:\n",
    "                    for mod_h in r_h[equal].split(\",\"):\n",
    "                        for mod_t in r_t[equal].split(\",\"):\n",
    "                            #print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                            if(relacion_entailment(mod_t,mod_h)):\n",
    "                                borrar.append(equal)\n",
    "                                c_compatibilidad+=1\n",
    "                else:\n",
    "                    borrar.append(str(b[j]))\n",
    "                    c_compatibilidad+=1\n",
    "        pasada+=1\n",
    "        ma = ma.drop(borrar,axis=1)\n",
    "        m_earth = m_earth.drop(borrar,axis=1)\n",
    "        m_mi = m_mi.drop(borrar,axis=1)\n",
    "        n_columns = ma.shape[1]\n",
    "        b_col.extend(borrar)\n",
    "    b_index=[0]\n",
    "    \n",
    "    # #   ALMACENAMIENTO DE TODA LA INFORMACIÓN PROCESADA DE CARACTERÍSTICAS\n",
    "    m_distancia = obtener_distancia(t_vectors,h_vectors,t_clean,h_clean,b_col,b_index)\n",
    "    \n",
    "    new_data['distancias'].append(m_distancia.max().sum()) #cambie de maximas a sumas\n",
    "    m_earth=m_earth*m_distancia\n",
    "    if ma.shape[1]==0:\n",
    "        new_data['entropias'].append(0)\n",
    "        new_data['max_info'].append(0)\n",
    "        new_data['sumas'].append(0)\n",
    "        new_data['mearts'].append(0)\n",
    "        new_data['mutinf'].append(0)\n",
    "        new_data['diferencias'].append(0)\n",
    "    else:\n",
    "        new_data['entropias'].append(entropia(ma.round(1).values.flatten()))\n",
    "        new_data['max_info'].append(ma.max().sum()/(ma.shape[1]))# \n",
    "        new_data['sumas'].append(ma.sum().sum()/(ma.shape[1]))# \n",
    "        new_data['mearts'].append(m_earth.min().sum()/(ma.shape[1]))# \n",
    "        new_data['mutinf'].append(m_mi.max().sum()/(ma.shape[1]))# \n",
    "        new_data['diferencias'].append(len(ma.columns)/len(ma.index))\n",
    "\n",
    "    new_data['list_comp'].append(c_compatibilidad)\n",
    "    new_data['list_incomp'].append(c_incompatibilidad)\n",
    "    # new_data['list_rel_con'].append(c_rel_concep)\n",
    "    # new_data['list_relaciones'].append(parejas)\n",
    "    # new_data['listas_malignf'].append(ma)\n",
    "    new_data['list_m'].append(ma.shape[1])\n",
    "    #new_data['clases'].append(prueba.at[i,\"gold_label\"])\n",
    "    new_data['clases'].append(1)\n",
    "    print(ma)\n",
    "fin = time.time()\n",
    "print(\"Tiempo que se llevo:\",round(fin-inicio,2),\" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sumas</th>\n",
       "      <th>distancias</th>\n",
       "      <th>entropia_total</th>\n",
       "      <th>entropias</th>\n",
       "      <th>mutinf</th>\n",
       "      <th>mearts</th>\n",
       "      <th>max_info</th>\n",
       "      <th>list_comp</th>\n",
       "      <th>diferencias</th>\n",
       "      <th>list_M</th>\n",
       "      <th>list_m</th>\n",
       "      <th>list_T</th>\n",
       "      <th>Jaro-Winkler_rit</th>\n",
       "      <th>clases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.439</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.737374</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.281016</td>\n",
       "      <td>2.784137</td>\n",
       "      <td>1.918</td>\n",
       "      <td>1.918</td>\n",
       "      <td>4.037431</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.892967</td>\n",
       "      <td>0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0.563889</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.935</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.394491</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sumas  distancias  entropia_total  entropias    mutinf    mearts  \\\n",
       "0  0.000000    0.000000           2.439      0.000  0.000000  0.000000   \n",
       "1  1.281016    2.784137           1.918      1.918  4.037431  0.001126   \n",
       "2  0.000000    0.000000           1.000      0.000  0.000000  0.000000   \n",
       "3  0.000000    0.000000           1.935      0.000  0.000000  0.000000   \n",
       "\n",
       "   max_info  list_comp  diferencias  list_M  list_m  list_T  Jaro-Winkler_rit  \\\n",
       "0  0.000000          3     0.000000       4       0       5          0.737374   \n",
       "1  0.892967          0     0.666667       2       2       3          0.563889   \n",
       "2  0.000000          2     0.000000       2       0       2          0.916667   \n",
       "3  0.000000          5     0.000000       3       0      18          0.394491   \n",
       "\n",
       "   clases  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultados = pd.DataFrame(new_data)\n",
    "df_resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
