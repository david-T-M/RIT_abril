{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/rit/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import utils as ut # esta librería tiene funciones para poder obtener un procesamiento del <T,H>\n",
    "import spacy\n",
    "import mutual_info as mi\n",
    "import time\n",
    "from scipy.stats import wasserstein_distance\n",
    "import sys\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import conceptnet_lite\n",
    "conceptnet_lite.connect(\"../OPENAI/data/conceptnet.db\")\n",
    "from conceptnet_lite import Label, edges_for, edges_between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_distancia(texto_v,hipotesis_v,texto_t,texto_h,b_col,b_index):\n",
    "    lista_l=[]\n",
    "    for i in range(len(texto_t)):\n",
    "        lista=[]\n",
    "        for j in range(len(texto_h)):\n",
    "            lista.append(np.linalg.norm(texto_v[i] - hipotesis_v[j]))#*wasserstein_distance(texto_2[i],hipotesis_2[j]))\n",
    "        lista_l.append(lista)\n",
    "    df_distEuc=pd.DataFrame(lista_l,index=texto_t,columns=texto_h)\n",
    "    df_distEuc=df_distEuc.drop(b_col[1:],axis=1)\n",
    "    df_distEuc=df_distEuc.drop(b_index[1:],axis=0)\n",
    "    return df_distEuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wasserstein_mutual_inf(texto_v,hipotesis_v,texto_t,texto_h):  \n",
    "    lista_l=[]\n",
    "    lista_muinfor=[]   \n",
    "    for i in range(len(texto_t)):\n",
    "        lista=[]\n",
    "        lista_mu=[]\n",
    "        for j in range(len(texto_h)):\n",
    "            lista.append(wasserstein_distance(texto_v[i],hipotesis_v[j]))\n",
    "            lista_mu.append(mi.mutual_information_2d(np.array(texto_v[i]),np.array(hipotesis_v[j])))\n",
    "        lista_l.append(lista)\n",
    "        lista_muinfor.append(lista_mu)\n",
    "    DFmearth=pd.DataFrame(lista_l,index=texto_t,columns=texto_h)\n",
    "    DFmutual_inf=pd.DataFrame(lista_muinfor,index=texto_t,columns=texto_h)\n",
    "    return DFmearth,DFmutual_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropia(X):\n",
    "    \"\"\"Devuelve el valor de entropia de una muestra de datos\"\"\" \n",
    "    probs = [np.mean(X == valor) for valor in set(X)]\n",
    "    return round(sum(-p * np.log2(p) for p in probs), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "relaciones_generales=[\"related_to\",\"is_a\",\"etymologically_related_to\",\"manner_of\",\"has_a\",\"derived_from\",\"has_property\",\"form_of\",\"causes\",\"has_prerequisite\",\"has_subevent\",\"has_first_subevent\"]\n",
    "relaciones_especificas=[\"is_a\",\"manner_of\",\"has_a\",\"derived_from\",\"has_property\",\"form_of\",\"causes\",\"has_prerequisite\",\"has_subevent\",\"has_first_subevent\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_synonyms(word):\n",
    "    sinonimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name == \"synonym\":\n",
    "                if word== e.start.text:\n",
    "                    sinonimos.add(e.end.text)\n",
    "                elif word== e.end.text:\n",
    "                    sinonimos.add(e.start.text)\n",
    "    except:\n",
    "        pass\n",
    "    sinonimos.add(word)\n",
    "    return sinonimos\n",
    "\n",
    "def bag_of_antonyms(word):\n",
    "    antonimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name in [\"antonym\",\"distinc_from\"]:\n",
    "                if word== e.start.text:\n",
    "                    antonimos.add(e.end.text)\n",
    "                elif word== e.end.text:\n",
    "                    antonimos.add(e.start.text)\n",
    "    except:\n",
    "        pass\n",
    "    return antonimos\n",
    "\n",
    "def bag_of_hyperonyms(word):\n",
    "    hiperonimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name in relaciones_generales:\n",
    "                if word== e.start.text:\n",
    "                    hiperonimos.add(e.end.text)\n",
    "    except:\n",
    "        pass\n",
    "    return hiperonimos\n",
    "\n",
    "def bag_of_hyponyms(word):\n",
    "    hiponimos=set()\n",
    "    try:\n",
    "        for e in edges_for(Label.get(text=word, language='en').concepts, same_language=True):\n",
    "            if e.relation.name in relaciones_especificas:\n",
    "                if word== e.end.text:\n",
    "                    hiponimos.add(e.start.text)\n",
    "                    #print(e.relation.name,e.start.text)\n",
    "    except:\n",
    "        pass\n",
    "    return hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_distanceDavid2(s1, s2,sinT,sinH,HipT,hipH) :\n",
    "    bandera=True\n",
    "\n",
    "    # Length of two strings\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "\n",
    "    # If the listas de tokens are equal \n",
    "    if len1==len2:\n",
    "        for i in range(len1):\n",
    "            if s1[i]!=s2[i]:\n",
    "                bandera=False\n",
    "                break\n",
    "        if (bandera):\n",
    "            return 1.0; \n",
    " \n",
    "    if (len1 == 0 or len2 == 0) :\n",
    "        return 0.0; \n",
    " \n",
    "    # Maximum distance upto which matching \n",
    "    # is allowed \n",
    "    max_dist = (max(len(s1), len(s2)) // 2 )-1 ; \n",
    " \n",
    "    # Count of matches \n",
    "    match = 0; \n",
    "    matchC = 0; \n",
    " \n",
    "    # Hash for matches \n",
    "    hash_s1 = [0] * len(s1)\n",
    "    hash_s2 = [0] * len(s2)\n",
    " \n",
    "    # Traverse through the first string \n",
    "    for j in range( max(0, i - max_dist),\n",
    "                    min(len2, i + max_dist + 1)) : \n",
    "            print(s1[i],s2[j])\n",
    "            # If there is a match or is contain in a bag of sinomys of tk\n",
    "            if ((s1[i] == s2[j] or s1[i] in sinH[j] or s2[j] in sinT[i]) and hash_s2[j] == 0) : \n",
    "                print(s1[i],s2[j],\"sinonimos\")\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s1[i] in hipH[j] or len((sinT[i]).intersection(hipH[j]))>0) and hash_s2[j] == 0):\n",
    "                print(\"hiperonimos\",s2[j],s1[i],(sinT[i]).intersection(hipH[j]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s2[j] in HipT[i] or len((sinH[j]).intersection(HipT[i]))>0) and hash_s2[j] == 0): \n",
    "                print(\"hiperonimos sobre sinonimos\",s2[j],s1[i])\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif len((hipH[j]).intersection(HipT[i]))>0 and hash_s2[j] == 0: \n",
    "                print(\"hiph HiperT\",s2[j],s1[i],hipH[j],HipT[i],(hipH[j]).intersection(HipT[i]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "    print(hash_s1)\n",
    "    print(hash_s2)\n",
    "    print(match)\n",
    "    # If there is no match \n",
    "    if (match == 0) :\n",
    "        return 0.0; \n",
    "    return match/len2 ,(len2-match-matchC)/len2; \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_distance(s1, s2,sinT,sinH,HipT,hipH) :\n",
    "    bandera=True\n",
    "\n",
    "    # Length of two strings\n",
    "    len1 = len(s1)\n",
    "    len2 = len(s2)\n",
    "\n",
    "    # If the listas de tokens are equal \n",
    "    if len1==len2:\n",
    "        for i in range(len1):\n",
    "            if s1[i]!=s2[i]:\n",
    "                bandera=False\n",
    "                break\n",
    "        if (bandera):\n",
    "            return 1.0; \n",
    " \n",
    "    if (len1 == 0 or len2 == 0) :\n",
    "        return 0.0; \n",
    " \n",
    "    # Maximum distance upto which matching \n",
    "    # is allowed \n",
    "    max_dist = (max(len(s1), len(s2)) // 2 ); \n",
    " \n",
    "    # Count of matches \n",
    "    match = 0; \n",
    " \n",
    "    # Hash for matches \n",
    "    hash_s1 = [0] * len(s1)\n",
    "    hash_s2 = [0] * len(s2)\n",
    " \n",
    "    # Traverse through the first string \n",
    "    for i in range(len1):\n",
    " \n",
    "        # Check if there is any matches\n",
    "        for j in range(max(0, i - max_dist), \n",
    "                       min(len2, i + max_dist + 1)):\n",
    "            print(s1[i],s2[j])\n",
    "            # If there is a match or is contain in a bag of sinomys of tk\n",
    "            if ((s1[i] == s2[j] or s1[i] in sinH[j] or s2[j] in sinT[i]) and hash_s2[j] == 0) : \n",
    "                print(s1[i],s2[j],\"sinonimos\")\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s1[i] in hipH[j] or len((sinT[i]).intersection(hipH[j]))>0) and hash_s2[j] == 0):\n",
    "                print(\"hiponimos\",s2[j],s1[i],(sinT[i]).intersection(hipH[j]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif ((s2[j] in HipT[i] or len((sinH[j]).intersection(HipT[i]))>0) and hash_s2[j] == 0): \n",
    "                print(\"hiperonimos sobre sinonimos\",s2[j],s1[i])\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            elif len((hipH[j]).intersection(HipT[i]))>0 and hash_s2[j] == 0: \n",
    "                print(\"hiperonimos3\",s2[j],s1[i],(hipH[j]).intersection(HipT[i]))\n",
    "                hash_s1[i] += 1; \n",
    "                hash_s2[j] += 1; \n",
    "                match += 1; \n",
    "                break\n",
    "            \n",
    "    print(hash_s1)\n",
    "    print(hash_s2)\n",
    "    print(match)\n",
    "    # If there is no match \n",
    "    if (match == 0) :\n",
    "        return 0.0; \n",
    " \n",
    "    # Number of transpositions \n",
    "    t = 0; \n",
    " \n",
    "    point = 0; \n",
    " \n",
    "    # Count number of occurrences \n",
    "    # where two characters match but \n",
    "    # there is a third matched character \n",
    "    # in between the indices \n",
    "    for i in range(len1) : \n",
    "        if (hash_s1[i]) :\n",
    "            # Find the next matched character \n",
    "            # in second string \n",
    "            while (hash_s2[point] == 0) :\n",
    "                point += 1; \n",
    " \n",
    "            if (s1[i] != s2[point]) :\n",
    "                point += 1\n",
    "                t += 1\n",
    "            else :\n",
    "                point += 1    \n",
    "    t /= 2; \n",
    "    #Return the Jaro Similarity \n",
    "    return (match/ len1 + match / len2 +\n",
    "            (match - t) / match)/ 3.0; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\") # modelo de nlp\n",
    "\n",
    "#ut.load_vectors_in_lang(nlp,\"../OPENAI/data/glove.840B.300d.txt\") # carga de vectores en nlp.wv\n",
    "ut.load_vectors_in_lang(nlp,\"./data/numberbatch-en-17.04b.txt\") # carga de vectores en nlp.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#prueba=pd.read_csv(\"data/DEV/pruebaDEV.csv\")\n",
    "#prueba=pd.read_csv(\"../OPENAI/data/\"+sys.argv[1])\n",
    "\n",
    "#textos = prueba[\"sentence1\"].to_list()       # almacenamiento en listas\n",
    "#hipotesis = prueba[\"sentence2\"].to_list()\n",
    "textos=[\"A man dressed in a red shirt and black tie stands up at a wedding reception to make a speech.\",\"Fishermen using poison sodium cyanide have helped destroy estimated 70 reefs\",\"A woman finishing a marathon race.\",\"Zapatero visited the following cities in four days: Brasilia, São Paulo, Buenos Aires and Santiago de Chile. According to official sources these visits are the last part of the project he began at the EU-Latin American Summit in Guadalajara, Mexico and pursued in the Ibero-American meeting in Costa Rica in November.\",\"The child is hanging upside down with his legs over a pole.\",\"Someone is falling off a horse\",\"As late as 1799, priests were still being imprisoned or deported to penal colonies and persecution only worsened after the French army led by General Louis Alexandre Berthier captured Rome and imprisoned Pope Pius VI, who would die in captivity in Valence, Drôme, France in August of 1799.\"]\n",
    "hipotesis= [\"a guy in a red top and tie makes a speech\",\"Cyanide fishing linked destruction area reefs\",\"The woman is not going to finish the race.\",\"Zapatero participated in the Ibero-American meeting in Costa Rica.\",\"There is an adult upside down.\",\"The man is not knocked off of a horse\",\"Alexandre Berthier died in 1799.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags=[ 'acl','acomp','advcl','advmod','amod','appos','ccomp','complm','compound','conj','infmod','meta','neg',\n",
    " 'nmod','nn','npadvmod','nounmod','npmod','num','number','nummod','partmod','pcomp','poss','possessive',\n",
    " #'prep',\n",
    " 'quantmod', 'rcmod', 'relcl', 'xcomp', 'adc', 'avc', 'mnr', 'mo', 'ng', 'nmc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion_entidades(nlp,texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    dir_sust=dict()\n",
    "    palabras=[]\n",
    "    for token in doc:\n",
    "        if token.dep_ in tags or \"mod\" in token.dep_ or \"comp\" in token.dep_ or \"neg\" in token.dep_:\n",
    "            #print(token.text, token.lemma_, token.pos_,token.dep_,token.head.text,token.head.lemma_, token.head.pos_,\n",
    "            #    [child for child in token.children])\n",
    "            #sustantivos.append(token.head.lemma_)\n",
    "            if token.head.lemma_ in dir_sust:\n",
    "                if dir_sust[token.head.lemma_]==\"NA\":\n",
    "                    dir_sust[token.head.lemma_]=token.lemma_\n",
    "                    if token.head not in palabras:\n",
    "                        palabras.append(token.head)\n",
    "                else:\n",
    "                    dir_sust[token.head.lemma_]=dir_sust[token.head.lemma_]+\",\"+token.lemma_\n",
    "                    if token.head not in palabras:\n",
    "                        palabras.append(token.head)\n",
    "            else:\n",
    "                dir_sust[token.head.lemma_]=token.lemma_\n",
    "                if token.head not in palabras:\n",
    "                    palabras.append(token.head)\n",
    "        elif token.pos_ in [\"NOUN\"]:\n",
    "            #print(token.text,token.lemma_, token.pos_,token.dep_)\n",
    "            #sustantivos.append(token.lemma_)\n",
    "            if token.lemma_ not in dir_sust:\n",
    "                dir_sust[token.lemma_]=\"NA\"\n",
    "                if token not in palabras:\n",
    "                    palabras.append(token)\n",
    "        elif token.pos_ in [\"VERB\"]:\n",
    "            #print(token.text, token.lemma_,token.pos_,token.dep_)\n",
    "            #sustantivos.append(token.lemma_)\n",
    "            if token.lemma_ not in dir_sust:\n",
    "                dir_sust[token.lemma_]=\"NA\"\n",
    "                if token not in palabras:\n",
    "                    palabras.append(token)\n",
    "    return dir_sust,palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'be': 'down', 'upside': 'adult', 'down': 'upside'}, [is, upside, down])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "representacion_entidades(nlp,\"There is an adult upside down.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representacion2(nlp,texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    dir_sust=dict()\n",
    "    palabras=[]\n",
    "    noun_phrase= [chunk.lemma_ for chunk in doc.noun_chunks]\n",
    "    verbs = [token.lemma_ for token in doc if token.pos_ == \"VERB\"]\n",
    "    for chunk in noun_phrase:\n",
    "        doc = nlp(chunk)\n",
    "        nue=[]\n",
    "        for token in doc:\n",
    "            if token.pos_!=\"DET\":\n",
    "                nue.append(token.lemma_)\n",
    "        if len(nue)>1:\n",
    "            dir_sust[nue[-1]]=','.join(nue[:-1])\n",
    "        else:\n",
    "            dir_sust[nue[-1]]=\"NA\"\n",
    "    # for chunk in doc.noun_chunks:\n",
    "    #     if chunk.root.lemma_!=chunk.lemma_:\n",
    "    #         dir_sust[chunk.root.lemma_]=','.join(chunk.lemma_.split()[:-1])\n",
    "    #     else:\n",
    "    #         dir_sust[chunk.root.lemma_]=\"NA\"\n",
    "    for v in verbs:\n",
    "        dir_sust[v]=\"NA\"\n",
    "    #representacion_entidades(nlp,texto)\n",
    "    palabras.extend([chunk.root.lemma_ for chunk in doc.noun_chunks])\n",
    "    palabras.extend(verbs)\n",
    "    return dir_sust,palabras\n",
    "\n",
    "def relacion_entailment(wt,wh):\n",
    "    try:\n",
    "        concepts_wt = Label.get(text=wt, language='en').concepts\n",
    "        concepts_wh = Label.get(text=wh, language='en').concepts\n",
    "        for e in edges_between(concepts_wt, concepts_wh):\n",
    "            if wt == e.start.text and e.relation.name in relaciones_generales:\n",
    "                print(e.start.text, \"-\", e.end.text, \"|\", e.relation.name,e)\n",
    "                return True\n",
    "    except:\n",
    "        pass\n",
    "    return False\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negacion(nlp,texto):\n",
    "    doc = nlp(texto.lower())\n",
    "    for token in doc:\n",
    "        if(token.dep_==\"neg\"):\n",
    "            return 1, token.head.lemma_\n",
    "    return 0,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relacion_noentailment(wt,wh):\n",
    "    try:\n",
    "        concepts_wt = Label.get(text=wt, language='en').concepts\n",
    "        concepts_wh = Label.get(text=wh, language='en').concepts\n",
    "        for e in edges_between(concepts_wt, concepts_wh):\n",
    "            if wt == e.start.text and e.relation.name in [\"distinct_from\",\"antonym\"]:\n",
    "                print(e.start.text, \"-\", e.end.text, \"|\", e.relation.name,e)\n",
    "                return True\n",
    "    except:\n",
    "        pass\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'sumas' : [], 'distancias' : [], 'entropia_total' : [],'entropias' : [],'mutinf' : [], \n",
    "            'mearts' : [], 'max_info' : [],  'list_comp' : [], 'diferencias' :[], 'list_incomp':[],\n",
    "            'list_M' : [], 'list_m' : [], 'list_T' : [], 'Jaro-Winkler_rit':[],\n",
    "            'negT' : [], 'verbT' : [], 'negH' : [], 'verbH':[], \n",
    "            'clases' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "A man dressed in a red shirt and black tie stands up at a wedding reception to make a speech.\n",
      "texto man NA\n",
      "texto shirt red\n",
      "texto tie black\n",
      "texto reception wedding\n",
      "texto speech NA\n",
      "texto dress NA\n",
      "texto stand NA\n",
      "texto make NA\n",
      "a guy in a red top and tie makes a speech\n",
      "hipotesis guy NA\n",
      "hipotesis top red\n",
      "hipotesis tie NA\n",
      "hipotesis speech NA\n",
      "hipotesis make NA\n",
      "clean\n",
      "['man', 'shirt', 'tie', 'reception', 'speech', 'dress', 'stand', 'make']\n",
      "['guy', 'top', 'tie', 'speech', 'make']\n",
      "['man', 'dress', 'red', 'shirt', 'black', 'tie', 'stand', 'wedding', 'reception', 'make', 'speech']\n",
      "['guy', 'red', 'top', 'tie', 'make', 'speech']\n",
      "man guy\n",
      "hiperonimos sobre sinonimos guy man\n",
      "dress guy\n",
      "dress red\n",
      "hiperonimos sobre sinonimos red dress\n",
      "red guy\n",
      "red red\n",
      "red top\n",
      "hiperonimos3 top red {'stop'}\n",
      "shirt guy\n",
      "shirt red\n",
      "shirt top\n",
      "shirt tie\n",
      "hiperonimos sobre sinonimos tie shirt\n",
      "black guy\n",
      "black red\n",
      "black top\n",
      "black tie\n",
      "black make\n",
      "black speech\n",
      "tie guy\n",
      "tie red\n",
      "tie top\n",
      "tie tie\n",
      "tie make\n",
      "hiperonimos sobre sinonimos make tie\n",
      "stand red\n",
      "stand top\n",
      "stand tie\n",
      "stand make\n",
      "stand speech\n",
      "wedding top\n",
      "wedding tie\n",
      "wedding make\n",
      "wedding speech\n",
      "reception tie\n",
      "reception make\n",
      "reception speech\n",
      "make make\n",
      "make speech\n",
      "speech speech\n",
      "speech speech sinonimos\n",
      "[1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "6\n",
      "['NA']\n",
      "['NA']\n",
      "man - guy | related_to /a/[/r/related_to/,/c/en/man/,/c/en/guy/]\n",
      "man guy True\n",
      "checar atributos\n",
      "['red']\n",
      "['red']\n",
      "shirt - top | is_a /a/[/r/is_a/,/c/en/shirt/n/,/c/en/top/n/opencyc/clothing_top/]\n",
      "shirt top True\n",
      "checar atributos\n",
      "lo que tiene: top red\n",
      "red red\n",
      "red red True\n",
      "tie tie True misma palabra\n",
      "speech speech True misma palabra\n",
      "make make True misma palabra\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [man, shirt, tie, reception, speech, dress, stand, make]\n",
      "1\n",
      "Fishermen using poison sodium cyanide have helped destroy estimated 70 reefs\n",
      "texto fisherman NA\n",
      "texto cyanide poison,sodium\n",
      "texto reef estimate,70\n",
      "texto use NA\n",
      "texto help NA\n",
      "texto destroy NA\n",
      "texto estimate NA\n",
      "Cyanide fishing linked destruction area reefs\n",
      "hipotesis fishing cyanide\n",
      "hipotesis reef destruction,area\n",
      "hipotesis link NA\n",
      "clean\n",
      "['fisherman', 'cyanide', 'reef', 'use', 'help', 'destroy', 'estimate']\n",
      "['fishing', 'reef', 'link']\n",
      "['fisherman', 'use', 'poison', 'sodium', 'cyanide', 'help', 'destroy', 'estimate', '70', 'reef']\n",
      "['cyanide', 'fishing', 'link', 'destruction', 'area', 'reef']\n",
      "fisherman cyanide\n",
      "fisherman fishing\n",
      "hiperonimos sobre sinonimos fishing fisherman\n",
      "use cyanide\n",
      "use fishing\n",
      "use link\n",
      "use destruction\n",
      "use area\n",
      "use reef\n",
      "poison cyanide\n",
      "hiperonimos sobre sinonimos cyanide poison\n",
      "sodium cyanide\n",
      "sodium fishing\n",
      "sodium link\n",
      "sodium destruction\n",
      "sodium area\n",
      "sodium reef\n",
      "cyanide cyanide\n",
      "cyanide fishing\n",
      "cyanide link\n",
      "cyanide destruction\n",
      "cyanide area\n",
      "cyanide reef\n",
      "help cyanide\n",
      "help fishing\n",
      "help link\n",
      "help destruction\n",
      "help area\n",
      "help reef\n",
      "destroy fishing\n",
      "destroy link\n",
      "destroy destruction\n",
      "hiponimos destruction destroy {'ruin', 'kill', 'ravage'}\n",
      "estimate link\n",
      "estimate destruction\n",
      "estimate area\n",
      "estimate reef\n",
      "70 destruction\n",
      "70 area\n",
      "70 reef\n",
      "reef area\n",
      "reef reef\n",
      "reef reef sinonimos\n",
      "[1, 0, 1, 0, 0, 0, 1, 0, 0, 1]\n",
      "[1, 1, 0, 1, 0, 1]\n",
      "4\n",
      "['NA']\n",
      "['cyanide']\n",
      "fisherman - fishing | related_to /a/[/r/related_to/,/c/en/fisherman/n/,/c/en/fishing/]\n",
      "fisherman fishing True\n",
      "checar atributos\n",
      "lo que tiene: fishing cyanide\n",
      "reef reef True misma palabra\n",
      "['NA']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "use link Hiperonimos y sinonimos {'member', 'tie'}\n",
      "checar atributos\n",
      "            fishing      reef\n",
      "fisherman  0.785934  0.355217\n",
      "cyanide    0.026225  0.036029\n",
      "reef       0.358626  1.000000\n",
      "use        0.044270 -0.032935\n",
      "help      -0.042190 -0.006249\n",
      "destroy   -0.002071  0.067065\n",
      "estimate  -0.046305 -0.017798\n",
      "2\n",
      "A woman finishing a marathon race.\n",
      "texto woman NA\n",
      "texto race marathon\n",
      "texto finish NA\n",
      "The woman is not going to finish the race.\n",
      "hipotesis woman NA\n",
      "hipotesis race NA\n",
      "hipotesis go NA\n",
      "hipotesis finish NA\n",
      "clean\n",
      "['woman', 'race', 'finish']\n",
      "['woman', 'race', 'go', 'finish']\n",
      "['woman', 'finish', 'marathon', 'race']\n",
      "['woman', 'not', 'go', 'finish', 'race']\n",
      "woman woman\n",
      "woman woman sinonimos\n",
      "finish woman\n",
      "finish not\n",
      "finish go\n",
      "finish finish\n",
      "finish finish sinonimos\n",
      "marathon woman\n",
      "marathon not\n",
      "marathon go\n",
      "marathon finish\n",
      "marathon race\n",
      "hiponimos race marathon {'marathon'}\n",
      "race not\n",
      "hiperonimos3 not race {'people'}\n",
      "[1, 1, 1, 1]\n",
      "[1, 1, 0, 1, 1]\n",
      "4\n",
      "woman woman True misma palabra\n",
      "race race True misma palabra\n",
      "['NA']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "finish go Hiperonimos y sinonimos {'travel', 'fare', 'journey', 'proceed', 'attempt', 'function', 'tour'}\n",
      "checar atributos\n",
      "finish finish True misma palabra\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [woman, race, finish]\n",
      "3\n",
      "Zapatero visited the following cities in four days: Brasilia, São Paulo, Buenos Aires and Santiago de Chile. According to official sources these visits are the last part of the project he began at the EU-Latin American Summit in Guadalajara, Mexico and pursued in the Ibero-American meeting in Costa Rica in November.\n",
      "texto zapatero NA\n",
      "texto city follow\n",
      "texto day four\n",
      "texto brasilia NA\n",
      "texto paulo são\n",
      "texto buenos NA\n",
      "texto chile santiago,de\n",
      "texto source official\n",
      "texto visit NA\n",
      "texto part last\n",
      "texto project NA\n",
      "texto he NA\n",
      "texto summit eu,-,latin,american\n",
      "texto guadalajara NA\n",
      "texto mexico NA\n",
      "texto meeting ibero,-,american\n",
      "texto rica costa\n",
      "texto november NA\n",
      "texto follow NA\n",
      "texto air NA\n",
      "texto accord NA\n",
      "texto begin NA\n",
      "texto pursue NA\n",
      "Zapatero participated in the Ibero-American meeting in Costa Rica.\n",
      "hipotesis zapatero NA\n",
      "hipotesis meeting ibero,-,american\n",
      "hipotesis rica costa\n",
      "hipotesis participate NA\n",
      "clean\n",
      "['zapatero', 'city', 'day', 'brasilia', 'paulo', 'buenos', 'chile', 'source', 'visit', 'part', 'project', 'he', 'summit', 'guadalajara', 'mexico', 'meeting', 'rica', 'november', 'follow', 'air', 'accord', 'begin', 'pursue']\n",
      "['zapatero', 'meeting', 'rica', 'participate']\n",
      "['zapatero', 'visit', 'follow', 'city', 'four', 'day', 'brasilia', 'são', 'paulo', 'buenos', 'air', 'santiago', 'chile', 'accord', 'official', 'source', 'visit', 'last', 'part', 'project', 'begin', 'eu', 'latin', 'american', 'summit', 'guadalajara', 'mexico', 'pursue', 'ibero', 'american', 'meeting', 'costa', 'rica', 'november']\n",
      "['zapatero', 'participate', 'ibero', 'american', 'meeting', 'costa', 'rica']\n",
      "zapatero zapatero\n",
      "zapatero zapatero sinonimos\n",
      "visit zapatero\n",
      "visit participate\n",
      "visit ibero\n",
      "visit american\n",
      "visit meeting\n",
      "hiponimos meeting visit {'visit'}\n",
      "follow zapatero\n",
      "follow participate\n",
      "follow ibero\n",
      "follow american\n",
      "follow meeting\n",
      "follow costa\n",
      "follow rica\n",
      "city zapatero\n",
      "city participate\n",
      "city ibero\n",
      "city american\n",
      "city meeting\n",
      "city costa\n",
      "city rica\n",
      "four zapatero\n",
      "four participate\n",
      "four ibero\n",
      "four american\n",
      "four meeting\n",
      "four costa\n",
      "four rica\n",
      "day zapatero\n",
      "day participate\n",
      "day ibero\n",
      "day american\n",
      "day meeting\n",
      "day costa\n",
      "day rica\n",
      "brasilia zapatero\n",
      "brasilia participate\n",
      "brasilia ibero\n",
      "brasilia american\n",
      "brasilia meeting\n",
      "brasilia costa\n",
      "brasilia rica\n",
      "são zapatero\n",
      "são participate\n",
      "são ibero\n",
      "são american\n",
      "são meeting\n",
      "são costa\n",
      "são rica\n",
      "paulo zapatero\n",
      "paulo participate\n",
      "paulo ibero\n",
      "paulo american\n",
      "paulo meeting\n",
      "paulo costa\n",
      "paulo rica\n",
      "buenos zapatero\n",
      "buenos participate\n",
      "buenos ibero\n",
      "buenos american\n",
      "buenos meeting\n",
      "buenos costa\n",
      "buenos rica\n",
      "air zapatero\n",
      "air participate\n",
      "air ibero\n",
      "air american\n",
      "air meeting\n",
      "air costa\n",
      "air rica\n",
      "santiago zapatero\n",
      "santiago participate\n",
      "santiago ibero\n",
      "santiago american\n",
      "santiago meeting\n",
      "santiago costa\n",
      "santiago rica\n",
      "chile zapatero\n",
      "chile participate\n",
      "chile ibero\n",
      "chile american\n",
      "chile meeting\n",
      "chile costa\n",
      "chile rica\n",
      "accord zapatero\n",
      "accord participate\n",
      "accord ibero\n",
      "accord american\n",
      "accord meeting\n",
      "accord costa\n",
      "accord rica\n",
      "official zapatero\n",
      "official participate\n",
      "official ibero\n",
      "official american\n",
      "official meeting\n",
      "official costa\n",
      "official rica\n",
      "source zapatero\n",
      "source participate\n",
      "source ibero\n",
      "source american\n",
      "source meeting\n",
      "source costa\n",
      "source rica\n",
      "visit zapatero\n",
      "visit participate\n",
      "visit ibero\n",
      "visit american\n",
      "visit meeting\n",
      "visit costa\n",
      "visit rica\n",
      "last zapatero\n",
      "last participate\n",
      "last ibero\n",
      "last american\n",
      "last meeting\n",
      "last costa\n",
      "last rica\n",
      "part participate\n",
      "part ibero\n",
      "part american\n",
      "part meeting\n",
      "part costa\n",
      "part rica\n",
      "project ibero\n",
      "project american\n",
      "project meeting\n",
      "project costa\n",
      "project rica\n",
      "begin american\n",
      "begin meeting\n",
      "begin costa\n",
      "begin rica\n",
      "eu meeting\n",
      "eu costa\n",
      "eu rica\n",
      "latin costa\n",
      "latin rica\n",
      "american rica\n",
      "[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 0, 0, 1, 0, 0]\n",
      "2\n",
      "zapatero zapatero True misma palabra\n",
      "meeting meeting True misma palabra\n",
      "ibero ibero la misma\n",
      "- - la misma\n",
      "american - american | is_a /a/[/r/is_a/,/c/en/american/n/,/c/en/american/n/]\n",
      "american - american | is_a /a/[/r/is_a/,/c/en/american/n/,/c/en/american/n/]\n",
      "american american True\n",
      "rica rica True misma palabra\n",
      "costa costa la misma\n",
      "['last']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "part participate Hiperonimos y sinonimos {'partake', 'participate'}\n",
      "checar atributos\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [zapatero, city, day, brasilia, paulo, buenos, chile, source, visit, part, project, he, summit, guadalajara, mexico, meeting, rica, november, follow, air, accord, begin, pursue]\n",
      "4\n",
      "The child is hanging upside down with his legs over a pole.\n",
      "texto child NA\n",
      "texto leg his\n",
      "texto pole NA\n",
      "texto hang NA\n",
      "There is an adult upside down.\n",
      "hipotesis be NA\n",
      "clean\n",
      "['child', 'leg', 'pole', 'hang']\n",
      "['be']\n",
      "['child', 'hang', 'upside', 'down', 'leg', 'pole']\n",
      "['adult', 'upside', 'down']\n",
      "child adult\n",
      "hiperonimos sobre sinonimos adult child\n",
      "hang adult\n",
      "hang upside\n",
      "hiperonimos sobre sinonimos upside hang\n",
      "upside adult\n",
      "upside upside\n",
      "upside down\n",
      "down adult\n",
      "down upside\n",
      "down down\n",
      "down down sinonimos\n",
      "leg upside\n",
      "leg down\n",
      "pole down\n",
      "[1, 1, 0, 1, 0, 0]\n",
      "[1, 1, 1]\n",
      "3\n",
      "['NA']\n",
      "['NA']\n",
      "hang - be | manner_of /a/[/r/manner_of/,/c/en/hang/v/wn/stative/,/c/en/be/v/wn/stative/]\n",
      "hang be True\n",
      "checar atributos\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [child, leg, pole, hang]\n",
      "5\n",
      "Someone is falling off a horse\n",
      "texto someone NA\n",
      "texto horse NA\n",
      "texto fall NA\n",
      "The man is not knocked off of a horse\n",
      "hipotesis man NA\n",
      "hipotesis horse NA\n",
      "hipotesis knock NA\n",
      "clean\n",
      "['someone', 'horse', 'fall']\n",
      "['man', 'horse', 'knock']\n",
      "['fall', 'horse']\n",
      "['man', 'not', 'knock', 'horse']\n",
      "fall man\n",
      "fall not\n",
      "hiperonimos sobre sinonimos not fall\n",
      "horse man\n",
      "hiperonimos3 man horse {'white', 'father', 'black', 'godfather'}\n",
      "[1, 1]\n",
      "[1, 1, 0, 0]\n",
      "2\n",
      "['NA']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "someone man conjuntos de sinonimos {'person'}\n",
      "checar atributos\n",
      "horse horse True misma palabra\n",
      "['NA']\n",
      "['NA']\n",
      "Proceso de conjuntos\n",
      "fall knock conjuntos de sinonimos {'rap'}\n",
      "checar atributos\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [someone, horse, fall]\n",
      "6\n",
      "As late as 1799, priests were still being imprisoned or deported to penal colonies and persecution only worsened after the French army led by General Louis Alexandre Berthier captured Rome and imprisoned Pope Pius VI, who would die in captivity in Valence, Drôme, France in August of 1799.\n",
      "texto priest NA\n",
      "texto colony penal\n",
      "texto persecution NA\n",
      "texto army french\n",
      "texto rome NA\n",
      "texto vi pope,pius\n",
      "texto who NA\n",
      "texto captivity NA\n",
      "texto valence NA\n",
      "texto drôme NA\n",
      "texto france NA\n",
      "texto august NA\n",
      "texto imprison NA\n",
      "texto deport NA\n",
      "texto worsen NA\n",
      "texto lead NA\n",
      "texto capture NA\n",
      "texto die NA\n",
      "Alexandre Berthier died in 1799.\n",
      "hipotesis die NA\n",
      "clean\n",
      "['priest', 'colony', 'persecution', 'army', 'rome', 'vi', 'who', 'captivity', 'valence', 'drôme', 'france', 'august', 'imprison', 'deport', 'worsen', 'lead', 'capture', 'die']\n",
      "['die']\n",
      "['as', 'late', '1799', 'priest', 'still', 'imprison', 'deport', 'penal', 'colony', 'persecution', 'only', 'worsen', 'french', 'army', 'lead', 'general', 'louis', 'alexandre', 'berthier', 'capture', 'rome', 'imprison', 'pope', 'pius', 'vi', 'die', 'captivity', 'valence', 'drôme', 'france', 'august', '1799']\n",
      "['alexandre', 'berthi', 'die', '1799']\n",
      "as alexandre\n",
      "as berthi\n",
      "as die\n",
      "as 1799\n",
      "late alexandre\n",
      "late berthi\n",
      "late die\n",
      "late 1799\n",
      "1799 alexandre\n",
      "1799 berthi\n",
      "1799 die\n",
      "1799 1799\n",
      "1799 1799 sinonimos\n",
      "priest alexandre\n",
      "priest berthi\n",
      "priest die\n",
      "priest 1799\n",
      "still alexandre\n",
      "still berthi\n",
      "still die\n",
      "still 1799\n",
      "imprison alexandre\n",
      "imprison berthi\n",
      "imprison die\n",
      "imprison 1799\n",
      "deport alexandre\n",
      "deport berthi\n",
      "deport die\n",
      "deport 1799\n",
      "penal alexandre\n",
      "penal berthi\n",
      "penal die\n",
      "penal 1799\n",
      "colony alexandre\n",
      "colony berthi\n",
      "colony die\n",
      "hiperonimos3 die colony {'live', 'living'}\n",
      "persecution alexandre\n",
      "persecution berthi\n",
      "persecution die\n",
      "persecution 1799\n",
      "only alexandre\n",
      "only berthi\n",
      "only die\n",
      "only 1799\n",
      "worsen alexandre\n",
      "worsen berthi\n",
      "worsen die\n",
      "worsen 1799\n",
      "french alexandre\n",
      "french berthi\n",
      "french die\n",
      "french 1799\n",
      "army alexandre\n",
      "army berthi\n",
      "army die\n",
      "army 1799\n",
      "lead alexandre\n",
      "lead berthi\n",
      "lead die\n",
      "lead 1799\n",
      "general alexandre\n",
      "general berthi\n",
      "general die\n",
      "general 1799\n",
      "louis alexandre\n",
      "louis berthi\n",
      "louis die\n",
      "louis 1799\n",
      "alexandre berthi\n",
      "alexandre die\n",
      "alexandre 1799\n",
      "berthier die\n",
      "berthier 1799\n",
      "capture 1799\n",
      "[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1]\n",
      "2\n",
      "die die True misma palabra\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [priest, colony, persecution, army, rome, vi, who, captivity, valence, drôme, france, august, imprison, deport, worsen, lead, capture, die]\n",
      "Tiempo que se llevo: 354.9  segundos\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "for i in range(len(textos)):\n",
    "    print(i)\n",
    "    print(textos[i])\n",
    "    #r_t,t_clean_m=representacion_entidades(nlp,textos[i])\n",
    "    r_t,t_clean_m=representacion2(nlp,textos[i])\n",
    "    neg_t,negadat=negacion(nlp,textos[i])\n",
    "    new_data['negT'].append(neg_t)\n",
    "    new_data['verbT'].append(negadat)\n",
    "    #print(list(r_t.keys()))\n",
    "    for clave in r_t.keys():\n",
    "        print(\"texto\",clave,r_t[clave])\n",
    "    print(hipotesis[i])\n",
    "    #r_h,h_clean_m = representacion_entidades(nlp,hipotesis[i])\n",
    "    r_h,h_clean_m = representacion2(nlp,hipotesis[i])\n",
    "    neg_h,negadah=negacion(nlp,hipotesis[i])\n",
    "    new_data['negH'].append(neg_h)\n",
    "    new_data['verbH'].append(negadah)\n",
    "    for clave in r_h.keys():\n",
    "        print(\"hipotesis\",clave,r_h[clave])\n",
    "        \n",
    "    #print(list(r_h.keys()))\n",
    "    # t_clean=' '.join(list(r_t.keys()))\n",
    "    # h_clean=' '.join(list(r_h.keys()))\n",
    "    t_clean=list(r_t.keys())\n",
    "    h_clean=list(r_h.keys())\n",
    "    print(\"clean\")\n",
    "    print(t_clean)\n",
    "    print(h_clean)\n",
    "    # t_clean_m=ut.get_words(textos[i],nlp,pos_to_remove=['PUNCT'], normed=True,lemmatize=False)\n",
    "    # h_clean_m=ut.get_words(hipotesis[i],nlp,pos_to_remove=['PUNCT'], normed=True,lemmatize=False)\n",
    "    # print(\"get_words\")\n",
    "    # print(\"clean_m\")\n",
    "    # print(t_clean_m)\n",
    "    # print(h_clean_m)\n",
    "    # t_clean_m=ut.reform_sentence2(t_clean_m,nlp)\n",
    "    # h_clean_m=ut.reform_sentence2(h_clean_m,nlp)\n",
    "    # print(\"reform2\")\n",
    "    # print(t_clean_m)\n",
    "    # print(h_clean_m)\n",
    "    # t_clean=ut.get_words_rep(t_clean_m,nlp,pos_to_remove=[\"\"],normed=True,lemmatize=False)\n",
    "    # h_clean=ut.get_words_rep(h_clean_m,nlp,pos_to_remove=[\"\"],normed=True,lemmatize=False)\n",
    "    # print(\"t_clean\")\n",
    "    # print(t_clean)\n",
    "    # print(h_clean)\n",
    "    # print(\"clean_m\")\n",
    "    # print(t_clean_m)\n",
    "    # print(h_clean_m)\n",
    "    t_vectors=ut.get_matrix_rep2(t_clean, nlp, normed=False)\n",
    "    h_vectors=ut.get_matrix_rep2(h_clean, nlp, normed=False)\n",
    "    t_vectors_n=ut.get_matrix_rep2(t_clean, nlp, normed=True)\n",
    "    h_vectors_n=ut.get_matrix_rep2(h_clean, nlp, normed=True)\n",
    "    \n",
    "    # print(t_vectors)\n",
    "    # print(h_vectors)\n",
    "    # print(t_clean_m,h_clean_m)\n",
    "    \n",
    "    # s1=t_clean_m.split()\n",
    "    # s2=h_clean_m.split()\n",
    "    t_lem=ut.get_lemmas_(textos[i],nlp)\n",
    "    h_lem=ut.get_lemmas_(hipotesis[i],nlp)\n",
    "    s1=t_lem\n",
    "    s2=h_lem\n",
    "    \n",
    "    sinT=[]\n",
    "    antT=[]\n",
    "    HipT=[]\n",
    "    sinH=[]\n",
    "    antH=[]\n",
    "    HipH=[]\n",
    "    hipH=[]\n",
    "    \n",
    "    # # encontrar bolsa de sinonimos de cada token\n",
    "    for t in s1:\n",
    "        sinT.append(bag_of_synonyms(t))\n",
    "        HipT.append(bag_of_hyperonyms(t))\n",
    "\n",
    "    for h in s2:\n",
    "        sinH.append(bag_of_synonyms(h))\n",
    "        hipH.append(bag_of_hyponyms(h))\n",
    "    print(t_lem)\n",
    "    print(h_lem)\n",
    "    tp1=jaro_distance(t_lem, h_lem,sinT,sinH,HipT,hipH)\n",
    "    new_data['Jaro-Winkler_rit'].append(tp1)\n",
    "    # #new_data['c_estructura'].append(tp2)\n",
    "    #new_data['nomatch'].append(tp3)\n",
    "    \n",
    "    #tp1,tp2=jaro_distance_relacionadas(t_lem, h_lem,sinT,HipT,sinH,hipH)\n",
    "    #new_data['Jaro-Winkler_relacionadas'].append(tp1)\n",
    "    #new_data['c2_estructura'].append(tp2)\n",
    "\n",
    "    #tp1,tp2=jaro_distance_contra(t_lem, h_lem,antT,HipT,antH,HipH)\n",
    "    #new_data['Jaro-Winkler_contra'].append(tp1)\n",
    "    #new_data['c1_estructura'].append(tp2)\n",
    "\n",
    "    # # Obtencion de matriz de alineamiento, matriz de move earth y mutual information\n",
    "    ma=np.dot(t_vectors_n,h_vectors_n.T)\n",
    "    #print(t_clean,h_clean)\n",
    "    #print(len(t_vectors_n),len(h_vectors_n),len(t_clean),len(h_clean))\n",
    "    m_earth,m_mi=wasserstein_mutual_inf(t_vectors_n,h_vectors_n,t_clean,h_clean)\n",
    "    ma=pd.DataFrame(ma,index=t_clean,columns=h_clean)\n",
    "    #print(ma)\n",
    "\n",
    "    # # Calculamos la entropia inicial de la matriz de distancias coseno sobre tokens de T y H\n",
    "    new_data['entropia_total'].append(entropia(ma.round(1).values.flatten())) \n",
    "\n",
    "    # ###### BORRADO DE COSAS QUE NO OCUPO, SOLO NOS QUEDAMOS CON INFORMACIÓN DE TIPOS DE PALABRA: NOUN, VERB, ADJ Y ADV\n",
    "    # # TAMBIÉN OMITIMOS EL VERBO BE DEBIDO A QUE POR LO REGULAR SE UTILIZA COMO AUXILIAR Y ES UN VERBO COPULATIVO\n",
    "    # # sirve para construir la llamada predicación nominal del sujeto de una oración: \n",
    "    # # #el sujeto se une con este verbo a un complemento obligatorio llamado atributo que por lo general determina \n",
    "    # # alguna propiedad, estado o equivalencia del mismo, por ejemplo: \"Este plato es bueno\". \"Juan está casado\".\n",
    "    c_compatibilidad=0\n",
    "    c_incompatibilidad=0\n",
    "    # c_rel_concep=0\n",
    "    b_col=[0]\n",
    "\n",
    "    new_data['list_T'].append(ma.shape[0])\n",
    "    new_data['list_M'].append(ma.shape[1])\n",
    "    \n",
    "    # # #PARA REVISAR SI EXISTEN RELACIONES DE SIMILITUD SEMÁNTICA A TRAVÉS DEL USO DE CONCEPNET\n",
    "    \n",
    "    n_index = ma.shape[0]\n",
    "    n_columns = ma.shape[1]\n",
    "    pasada=0\n",
    "    #print(ma.index,ma.columns)\n",
    "    combinaciones=[]\n",
    "\n",
    "    iguales=set(ma.index).intersection(set(ma.columns))\n",
    "\n",
    "    while n_columns>0 and pasada<4:\n",
    "        borrar=[]\n",
    "        a = ma.idxmax().values\n",
    "        b = ma.columns\n",
    "        for j in range(len(a)):\n",
    "            #print(a[j]+\"_\"+b[j] not in combinaciones,a[j]+\"_\"+b[j],combinaciones)\n",
    "            if a[j]+\"_\"+b[j] not in combinaciones:\n",
    "                combinaciones.append(a[j]+\"_\"+b[j])\n",
    "                if(relacion_noentailment(a[j],b[j])):\n",
    "                    c_incompatibilidad+=1\n",
    "                elif a[j]==b[j]: ## Si son palabras identicas entonces no hacemos más proceso\n",
    "                    print(a[j],b[j],True,\"misma palabra\")\n",
    "                    if 'NA'!=r_h[b[j]]:\n",
    "                        for mod_h in r_h[b[j]].split(\",\"):\n",
    "                            for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                if(relacion_entailment(mod_t,mod_h)):\n",
    "                                    print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                    borrar.append(b[j])\n",
    "                                    c_compatibilidad+=1\n",
    "                                elif(mod_t==mod_h):\n",
    "                                    print(mod_t,mod_h,\"la misma\")\n",
    "                                    borrar.append(b[j])\n",
    "                                    c_compatibilidad+=1\n",
    "                    else:\n",
    "                        borrar.append(b[j])\n",
    "                        c_compatibilidad+=1\n",
    "                else:# en otro caso buscamos una relación de generalidad\n",
    "                    print(r_t[a[j]].split(\",\"))\n",
    "                    print(r_h[b[j]].split(\",\"))\n",
    "                    r_e = relacion_entailment(a[j],b[j])\n",
    "                    if r_e:\n",
    "                        print(a[j],b[j], r_e)\n",
    "                        print(\"checar atributos\")\n",
    "                        if 'NA' !=r_h[b[j]]:\n",
    "                            print(\"lo que tiene:\",b[j],r_h[b[j]])\n",
    "                            for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                if 'NA'!=r_t[a[j]]:\n",
    "                                    for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                        print(mod_h,mod_t)\n",
    "                                        if mod_h==mod_t:\n",
    "                                            print(mod_t,mod_h,True)\n",
    "                                            borrar.append(b[j])\n",
    "                                            c_compatibilidad+=1\n",
    "                                        else:\n",
    "                                            if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                borrar.append(b[j])\n",
    "                                                c_compatibilidad+=1\n",
    "                        else:\n",
    "                            borrar.append(b[j])\n",
    "                            c_compatibilidad+=1\n",
    "                    else:\n",
    "                        print(\"Proceso de conjuntos\")\n",
    "                        sin1=bag_of_synonyms(a[j])\n",
    "                        sin2=bag_of_synonyms(b[j])\n",
    "                        if len(sin1.intersection(sin2))>0:\n",
    "                            print(a[j],b[j],\"conjuntos de sinonimos\",sin1.intersection(sin2))\n",
    "                            print(\"checar atributos\")\n",
    "                            if 'NA'!=r_h[b[j]]:\n",
    "                                print(\"lo que tiene:\",b[j],r_h[b[j]])\n",
    "                                for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                    if 'NA'!=r_t[a[j]]:\n",
    "                                        for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                            print(mod_h,mod_t)\n",
    "                                            if mod_h==mod_t:\n",
    "                                                print(mod_t,mod_h,True)\n",
    "                                                borrar.append(b[j])\n",
    "                                                c_compatibilidad+=1\n",
    "                                            else:\n",
    "                                                if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                    print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                    borrar.append(b[j])\n",
    "                                                    c_compatibilidad+=1\n",
    "                            else:\n",
    "                                borrar.append(b[j])\n",
    "                                c_compatibilidad+=1\n",
    "                        else:\n",
    "                            Hip1=set()\n",
    "                            for e in list(sin1):\n",
    "                                Hip1=Hip1.union(bag_of_hyperonyms(e))\n",
    "                            if len(Hip1.intersection(sin2))>0:\n",
    "                                print(a[j],b[j],\"Hiperonimos y sinonimos\",Hip1.intersection(sin2))\n",
    "                                print(\"checar atributos\")\n",
    "                                if 'NA'!=r_h[b[j]]:\n",
    "                                    print(\"lo que tiene:\",b[j],r_h[b[j]])\n",
    "                                    for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                        if 'NA'!=r_t[a[j]]:\n",
    "                                            for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                                print(mod_h,mod_t)\n",
    "                                                if mod_h==mod_t:\n",
    "                                                    print(mod_t,mod_h,True)\n",
    "                                                    borrar.append(b[j])\n",
    "                                                    c_compatibilidad+=1\n",
    "                                                else:\n",
    "                                                    if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                        print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                        borrar.append(b[j])\n",
    "                                                        c_compatibilidad+=1\n",
    "                                else:\n",
    "                                    borrar.append(b[j])\n",
    "                                    c_compatibilidad+=1\n",
    "                            else:\n",
    "                                hip2=set()\n",
    "                                for e in list(sin2):\n",
    "                                    hip2=hip2.union(bag_of_hyponyms(e))\n",
    "                                if len(sin1.intersection(hip2))>0:\n",
    "                                    print(a[j],b[j],\"Sinonimos e hiponimos\",sin1.intersection(hip2))\n",
    "                                    print(\"checar atributos\")\n",
    "                                    if 'NA'!=r_h[b[j]]:\n",
    "                                        for mod_h in r_h[b[j]].split(\",\"):\n",
    "                                            if 'NA'!=r_t[a[j]]:\n",
    "                                                for mod_t in r_t[a[j]].split(\",\"):\n",
    "                                                    #print(mod_h,mod_t)\n",
    "                                                    if mod_h==mod_t:\n",
    "                                                        #print(mod_t,mod_h,True)\n",
    "                                                        borrar.append(b[j])\n",
    "                                                        c_compatibilidad+=1\n",
    "                                                    else:\n",
    "                                                        if(relacion_entailment(mod_t,mod_h)):\n",
    "                                                            #print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                                                            borrar.append(b[j])\n",
    "                                                            c_compatibilidad+=1\n",
    "                                    else:\n",
    "                                        borrar.append(b[j])\n",
    "                                        c_compatibilidad+=1\n",
    "            #else:\n",
    "                #print(a[j],b[j],\"ya se habia revisado\")\n",
    "        for equal in iguales:\n",
    "            if equal+\"_\"+equal not in combinaciones:\n",
    "                if 'NA'!=r_h[equal]:\n",
    "                    for mod_h in r_h[equal].split(\",\"):\n",
    "                        for mod_t in r_t[equal].split(\",\"):\n",
    "                            #print(mod_t,mod_h,relacion_entailment(mod_t,mod_h))\n",
    "                            if(relacion_entailment(mod_t,mod_h)):\n",
    "                                borrar.append(equal)\n",
    "                                c_compatibilidad+=1\n",
    "                else:\n",
    "                    borrar.append(str(b[j]))\n",
    "                    c_compatibilidad+=1\n",
    "        pasada+=1\n",
    "        ma = ma.drop(borrar,axis=1)\n",
    "        m_earth = m_earth.drop(borrar,axis=1)\n",
    "        m_mi = m_mi.drop(borrar,axis=1)\n",
    "        n_columns = ma.shape[1]\n",
    "        b_col.extend(borrar)\n",
    "    b_index=[0]\n",
    "    \n",
    "    # #   ALMACENAMIENTO DE TODA LA INFORMACIÓN PROCESADA DE CARACTERÍSTICAS\n",
    "    m_distancia = obtener_distancia(t_vectors,h_vectors,t_clean,h_clean,b_col,b_index)\n",
    "    \n",
    "    new_data['distancias'].append(m_distancia.max().sum()) #cambie de maximas a sumas\n",
    "    m_earth=m_earth*m_distancia\n",
    "    if ma.shape[1]==0:\n",
    "        new_data['entropias'].append(0)\n",
    "        new_data['max_info'].append(0)\n",
    "        new_data['sumas'].append(0)\n",
    "        new_data['mearts'].append(0)\n",
    "        new_data['mutinf'].append(0)\n",
    "        new_data['diferencias'].append(0)\n",
    "    else:\n",
    "        new_data['entropias'].append(entropia(ma.round(1).values.flatten()))\n",
    "        new_data['max_info'].append(ma.max().sum()/(ma.shape[1]))# \n",
    "        new_data['sumas'].append(ma.sum().sum()/(ma.shape[1]))# \n",
    "        new_data['mearts'].append(m_earth.min().sum()/(ma.shape[1]))# \n",
    "        new_data['mutinf'].append(m_mi.max().sum()/(ma.shape[1]))# \n",
    "        new_data['diferencias'].append(len(ma.columns)/len(ma.index))\n",
    "\n",
    "    new_data['list_comp'].append(c_compatibilidad)\n",
    "    new_data['list_incomp'].append(c_incompatibilidad)\n",
    "    # new_data['list_rel_con'].append(c_rel_concep)\n",
    "    # new_data['list_relaciones'].append(parejas)\n",
    "    # new_data['listas_malignf'].append(ma)\n",
    "    new_data['list_m'].append(ma.shape[1])\n",
    "    #new_data['clases'].append(prueba.at[i,\"gold_label\"])\n",
    "    new_data['clases'].append(1)\n",
    "    print(ma)\n",
    "fin = time.time()\n",
    "print(\"Tiempo que se llevo:\",round(fin-inicio,2),\" segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sumas</th>\n",
       "      <th>distancias</th>\n",
       "      <th>entropia_total</th>\n",
       "      <th>entropias</th>\n",
       "      <th>mutinf</th>\n",
       "      <th>mearts</th>\n",
       "      <th>max_info</th>\n",
       "      <th>list_comp</th>\n",
       "      <th>diferencias</th>\n",
       "      <th>list_incomp</th>\n",
       "      <th>list_M</th>\n",
       "      <th>list_m</th>\n",
       "      <th>list_T</th>\n",
       "      <th>Jaro-Winkler_rit</th>\n",
       "      <th>negT</th>\n",
       "      <th>verbT</th>\n",
       "      <th>negH</th>\n",
       "      <th>verbH</th>\n",
       "      <th>clases</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.337</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.709596</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.262909</td>\n",
       "      <td>2.883895</td>\n",
       "      <td>1.664</td>\n",
       "      <td>1.627</td>\n",
       "      <td>4.158247</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.892967</td>\n",
       "      <td>1</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0.563889</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>go</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0.364846</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.419</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>knock</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.411</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0.354167</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sumas  distancias  entropia_total  entropias    mutinf    mearts  \\\n",
       "0  0.000000    0.000000           2.337      0.000  0.000000  0.000000   \n",
       "1  1.262909    2.883895           1.664      1.627  4.158247  0.001126   \n",
       "2  0.000000    0.000000           2.000      0.000  0.000000  0.000000   \n",
       "3  0.000000    0.000000           2.016      0.000  0.000000  0.000000   \n",
       "4  0.000000    0.000000           1.500      0.000  0.000000  0.000000   \n",
       "5  0.000000    0.000000           2.419      0.000  0.000000  0.000000   \n",
       "6  0.000000    0.000000           1.411      0.000  0.000000  0.000000   \n",
       "\n",
       "   max_info  list_comp  diferencias  list_incomp  list_M  list_m  list_T  \\\n",
       "0  0.000000          5     0.000000            0       5       0       8   \n",
       "1  0.892967          1     0.285714            0       3       2       7   \n",
       "2  0.000000          4     0.000000            0       4       0       3   \n",
       "3  0.000000          6     0.000000            0       4       0      23   \n",
       "4  0.000000          1     0.000000            0       1       0       4   \n",
       "5  0.000000          3     0.000000            0       3       0       3   \n",
       "6  0.000000          1     0.000000            0       1       0      18   \n",
       "\n",
       "   Jaro-Winkler_rit  negT verbT  negH  verbH  clases  \n",
       "0          0.709596     0           0              1  \n",
       "1          0.563889     0           0              1  \n",
       "2          0.850000     0           1     go       1  \n",
       "3          0.364846     0           0              1  \n",
       "4          0.722222     0           0              1  \n",
       "5          0.666667     0           1  knock       1  \n",
       "6          0.354167     0           0              1  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_resultados = pd.DataFrame(new_data)\n",
    "df_resultados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
